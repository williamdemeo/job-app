%% [25] L. de Moura and N. Bjørner. Efficient e-matching for SMT solvers. In F. Pfenning, editor,
%% Conference on Automated Deduction (CADE-21), volume 4603 of LNCS, pages
%% 183–198. Springer, 2007.
%% [62] D. Selsam and L. de Moura. Congruence closure in intensional type theory. In N. Olivetti
%% and A. Tiwari, editors, International Joint Conference on Automated Reasoning (IJCAR
%% 2016), volume 9706 of LNCS, pages 99–115. Springer, 2016.
%% [30] G. Ebner, S. Ullrich, J. Roesch, J. Avigad, and L. de Moura. A metaprogramming
%% framework for formal verification. In International Conference on Functional
%% Programming (ICFP 2017), pages 34:1–34:29. ACM, 2017.
%% [13] J. C. Blanchette, M. Haslbeck, D. Matichuk, and T. Nipkow. Mining the Archive of Formal
%% Proofs. In M. Kerber, editor, Conference on Intelligent Computer Mathematics (CICM
%% 2015), volume 9150 of LNCS, pages 1–15. Springer, 2015.
\documentclass[12pt]{amsart}  %[11pt]
\usepackage[utf8x]{inputenc}

\usepackage{xcolor}
\definecolor{keywordcolor}{rgb}{0.7, 0.1, 0.1}   % red
\definecolor{commentcolor}{rgb}{0.4, 0.4, 0.4}   % grey
\definecolor{symbolcolor}{rgb}{0.0, 0.1, 0.6}    % blue
\definecolor{sortcolor}{rgb}{0.1, 0.5, 0.1}      % green
\usepackage{comment}
\usepackage{listings}
\def\lstlanguagefiles{aux/lstlean.tex}
\lstset{language=lean}



\makeatletter
\def\@settitle{\begin{center}%
  \ifx\@subtitle\@empty\else
  \uppercasenonmath\@subtitle
     \footnotesize\mdseries\@subtitle     \\[1ex]
  \fi
  \baselineskip14\p@\relax
  \bfseries
  \uppercasenonmath\@title
  \@title
  \end{center}%
}
\def\subtitle#1{\gdef\@subtitle{#1}}
\def\@subtitle{}
\makeatother


%%%%%%% wjd: added these packages vvvvvvvvvvvvvvvvvvvvvvvvv
% PAGE GEOMETRY
% These settings are for letter format
%% \usepackage[margin=1.5in]{geometry}

%% \def\OPTpagesize{8.5in,11in}     % Page size
%% \def\OPTtopmargin{0.75in}     % Margin at the top of the page
%% \def\OPTbottommargin{0.75in}  % Margin at the bottom of the page
%% \def\OPTinnermargin{.5in}    % Margin on the inner side of the page
%% \def\OPTbindingoffset{0.35in} % Extra offset on the inner side
%% \def\OPToutermargin{0.5in}   % Margin on the outer side of the page
%% \usepackage[papersize={\OPTpagesize},
%%   top=\OPTtopmargin,
%%   bottom=\OPTbottommargin,
%%   inner=\OPTinnermargin,
%%   outer=\OPToutermargin
%%   twoside,
%%   includehead,
%%   bindingoffset=\OPTbindingoffset]{geometry}
\usepackage[margin=1.25in]{geometry}
\usepackage[yyyymmdd,hhmmss]{datetime}
\usepackage{background}
\backgroundsetup{
  position=current page.east,
  angle=-90,
  nodeanchor=east,
  vshift=-1cm,
  hshift=8cm,
  opacity=1,
  scale=1,
  contents={\textcolor{gray!80}{WORK IN PROGRESS.  DO NOT DISTRIBUTE. (compiled on \today\ at \currenttime)}}
}
%%%%%%  (end wjd addition of packages)

\usepackage{amsmath,amssymb,url,mathrsfs,enumerate,ifthen,xspace}
\usepackage{latexsym,amscd,amsthm,stmaryrd,scalefnt}
\usepackage{enumerate,latexsym,relsize,bussproofs}
\usepackage{aux/unixode}
\usepackage[colorlinks=true,urlcolor=black,linkcolor=black,citecolor=black]{hyperref}
%\usepackage{graphicx}
\usepackage{tikz}

\usepackage{aux/macros_master}

\input{aux/figures}

\newboolean{extver}
\setboolean{extver}{false}
%\setboolean{extver}{true}

\newcommand\extverurl{https://goo.gl/9mcWBi}
\newcommand\extverURL{https://github.com/williamdemeo/job-app/tree/master/research_statement/extended_version}


%% \usepackage{fancyhdr}
%% \pagestyle{fancy}
%% \lhead{{\it \workingtitle}} \chead{} \rhead{William DeMeo}
%% \lfoot{} \cfoot{\thepage}  \rfoot{}

%% \renewcommand{\headrulewidth}{0pt}
%% \renewcommand{\footrulewidth}{0pt}
%% \setlength{\footskip}{20pt}

%%% EDIT THIS %%%
\newcommand\workingtitle{{\Large Formal Foundations for}\\[8pt] {\Large Informal Mathematics Research}\\[20pt] {\large William DeMeo}}
\newcommand\workingsubtitle{}
%\newcommand\workingsubtitle{universal algebra and computer-aided theorem proving} %\\ Universal Algebra and Lattice Theory}
% \newcommand\workingtitle{Practical Foundations for Universal Algebra and Lattice Theory}

\title[Formal Foundations for Informal Mathematics]{\workingtitle}
% :\\\workingsubtitle}%% Lean Universal Algebra:}
% \subtitle{Research Program} %formalized universal algebra and \\computer-aided mathematical proof}

% \author{\large William D\protect{e}Meo}


\begin{document}

\maketitle

\clearpage



\null


\vskip5cm

{\small 
\begin{quote}
  % ``There are certain dangers inherent in the formalization of mathematics.
  ``Systems of axioms acquire a certain sanctity with age, and in the 
  \emph{how} of churning out theorems we forget \emph{why} we were studying 
  these conditions in the first place...
% Computer languages suffer far more from this problem:
% nobody would claim any intrinsic merit for \textsc{Fortran} or \textsc{Html}, but
% sheer weight of existing code keeps them in use. Through the need for a
% standard---\emph{any} standard---a similar disaster could befall mathematics
% if set theory were chosen. 
% As with any programming, and also with the
% verification of programs, far more detail is required than is customary in
% mathematics. G. H. Hardy (1940) claimed that there is no permanent
% place in the world for ugly mathematics, but I have never seen a program
% which is not ugly. 
Even when the mathematical context and formal
language are clear, we should not perpetuate old proofs but instead look
for new and more perspicuous ones.''\\
~ \phantom{x} \hfill
% --Paul Taylor~\cite[p.~45]{MR1694820}
--Paul Taylor~\cite{MR1694820}
\end{quote}
}


\clearpage




\begin{abstract}  
This document describes a research program, the primary goal of which 
is to develop and implement new theory and software libraries to support 
computer-aided mathematical proof in universal algebra, lattice theory, and 
category theory. 
A distinguishing feature of this effort is the high priority placed on 
\emph{usability} of the formal mathematical libraries that we produce.
We aim to codify the core definitions and theorems of our area
of expertise in a language that feels natural to working mathematicians 
with no special training in computer science. 
Thus our goal is a formal mathematical library that has the look and feel of the 
informal language in which most mathematicians are accustomed to working.

This research is part of a broader effort currently underway in a number of countries, carried out by disparate research groups with a common goal---to develop the next generation of \textbf{practical formal foundations for informal mathematics}, and to codify these foundations in a formal language that feels natural to mathematicians. In short, our goal is to present \emph{mathematics as it should be}.
\end{abstract}

\section{Introduction and Motivation}
A significant portion of the professional mathematician's time is typically occupied by tasks other than \textit{Deep Research}, by which we mean such activities as discovering truly non-trivial, publishable results, inventing novel proof techniques, or conceiving of compelling new research areas or programs. Indeed, consider how much time we spend looking for and fixing minor flaws in an argument, handling straightforward special cases of a long proof, or performing clever little derivations which, if we're honest, reduce to mere exercises that a capable graduate student could probably solve.  Add to this the time spent checking proofs when collaborating with others or reviewing journal submissions and it's safe to say that the time most of spend on Deep Research is fairly limited.

It may come as a surprise, then, that computer-aided theorem proving technology,
which is capable of managing much of the straight-forward and less interesting aspects of our work, has not found its way into mainstream mathematics. In fact, the reasons for this are simple to state, but difficult to resolve. For most mathematicians, the potential benefit of the currently available tools is outweighed by the time and patience required to learn how to put them to effective use.  The high upfront cost is due to the fact that most researchers carry out their work in a very efficient \emph{informal} dialect of mathematics---a common dialect that we share with our collaborators, and without which the effective proving and communicating of new mathematical results would be difficult, if not impossible.

One hopes that published mathematical results \emph{could} be translated into  the rigorous language of some system of logic and formally verified. Nonetheless, few would relish spending the substantial time and energy that such an exercise would require. A mathematician's job is to discover new theorems and to present them in a language that is rigorous enough to convince colleagues, yet informal enough to be efficient for developing new mathematics and for communicating these developments painlessly. Such a language is what we refer to as the \emph{Informal Language} of mathematics research. A relatively recent trend is challenging this status quo, however, and the number of mathematicians engaging in computer-aided mathematics research is on the rise. 

To verify mathematical arguments by computer, the arguments must be be written so that machines can interpret and check them. We refer to the practice of writing such proofs as \emph{interactive theorem proving}, and to the programming language and software system that checks such proofs as a \emph{proof assistant} or an \emph{interactive theorem prover}.  Such systems are increasingly used in academia and industry to verify the correctness of hardware, software, and protocols. \emph{Constructive type theory} and \emph{higher-order logics}, on which most modern proof assistants are based, have played significant roles in formalizing and mechanically confirming the solutions to important problems, such as~the \emph{Four-Color Theorem}~\cite{MR2463991} 
and the \emph{Feit-Thompson Odd Order Theorem}~\cite{gonthier:2013b}, as well as settling major open problems, such as the \emph{Kepler Conjecture}~\cite{MR3659768}.

Another justification for proof assistants is their potential for improving the referee and validation process.  The main issues here are human fallibility and the high opportunity cost of human talent. Indeed, a substantial amount of time and effort of talented individuals is expended on refereeing journal submissions. Despite this the typical review process concludes without supplying any guarantee of validity of the resulting publications (see, e.g.,~\cite{fasel:2017}).
Moreover, the emergence of large computational proofs (like Thomas Hales' proof of the Kepler Conjecture) lead to referee assignments that are impossible burdens on individual reviewers~\cite{heule:2017}. Worse than that, even when such work survives peer review, there remain disputes over correctness, completeness, whether nontrivial gaps exist, etc. Some recent examples include % Atiyah's claim to have proved the \emph{Riemann Hypothesis}, 
Zhuk's proof of the \emph{\csp dichotomy conjecture} of Feder and 
Vardi~\cite{zhuk:2017}, and Fasel's proposed solution to 
  \emph{Murthy's conjecture}~\cite{fasel:2017}.\footnote{Fasel's solution survived peer review 
  and was published in the \emph{Annals of Mathematics} before it emerged that a lemma was flawed and the entire proof collapsed~\cite{fasel:2017}.} 
By relying to a greater extent on computers to develop and check proofs, researchers can raise reviewing standards, even for computational proofs, thereby increasing confidence in the validity of new results. 

\begin{quote}
  ``The mathematical community today invests a good deal of time and resources in the refereeing process... surely any computational tools that can help in that regard should be valued.''\\[4pt]
  ~ \phantom{x} \hfill  --Jeremy Avigad
\end{quote}

As further justification for enlisting the help of computers for discovering and checking new mathematics, consider the space of all proofs comprehensible by the unassisted human mind, and then observe that this is but a small fraction of the collection of all potential mathematical proofs in the universe. The real consequences of this fact are becoming more apparent as mathematical discoveries reach the limits of our ability to confirm and publish them in a timely and cost effective way. Thus, it seems inevitable that proof assistants will have an increasingly important role to play in future mathematics research.

%% that the programming language technology required to liberate us from
%% these tedious tasks is now available.
%% Perhaps I am wrong about this. Perhaps exceptional mathematicians are like Gromov, who
%% recounts stretches of his career during which he focused seriously, each day from 9am to 9pm,
%% on deep mathematical problems. Indeed, it may be that
%% exceptional mathematicians consistently operate at the cutting edge of their field,
%% always skipping past the trivial steps that lead to new and striking results.
%% If that is true, then I am not an exceptional mathematician.
%% I suspect that on a good day I spend little more than an hour working
%% at the very heart of a problem, at the point where a true advance might be discovered.
%% However, despite the
%% trustworthiness guarantees they offer, most mathematicians find them too laborious to use.
% Thus, progress in programming language technology has presented us with
% an opportunity to liberate ourselves from much of the tedious busy work of mathematics.
% \emph{Proof assistants} (also known as \emph{interactive theorem provers}) are increasingly 
% used in academia and industry to verify the correctness of hardware, software, and protocols.
% However, this technology, in its current state, comes with substantial upfront costs,
% not the least of which is the time and energy required to learn a new 
% programming language. Evidently, most mathematicians find these costs prohibitive.
%% Indeed, this is a real risk;
%% if it turns out that the investment cost exceeds the eventual rewards,
%% this could seriously jeopardize one's career, especially when that demands frequent and
%% significant publications of new theorems.

% \emph{The main strength of a proof assistant is trustworthiness:} all definitions and lemmas are
% checked for well-formedness, and even the most trivial proof steps are verified by an inference
% kernel with respect to a logical system. It took a formal proof to dispel doubts about Hales's proof.
% Another remarkable achievement of proof technology is Leroy's verified compiler~\cite{MR2559087}, 
% which is set to become a key component of the Airbus Corporation's 
% toolchain.\footnote{Airbus relies on proof technology because with it they are able to produce \emph{reliably}
% optimized code. Without guarantees provided by a proof assistants, %such ``proof-carrying code,''
% such optimizations would not meet the strict certification standards 
% imposed by the authorities regulating applications where human safety is
% a concern, such as commercial aircraft design and construction.} %(See~\cite{blazy}.)


Beyond their importance as a means of establishing trust in mathematical results,
formal proofs can also expose and clarify difficult steps in an argument.
Even before one develops a formal proof, the mere act of expressing a theorem statement (including the foundational axioms, definitions, and hypotheses on which it depends) in a precise and (when possible) computable way almost always leads to a deeper understanding of the result. Moreover, by keeping track of changes across a collection of results (axioms, hypotheses, etc), proof assistants facilitate experiments with variations and generalizations. When changing a definition, a mathematician equipped with a proof assistant is alerted to the inferences that need repair, redundant definitions, unnecessary hypotheses, etc.

Finally, modern proof assistants support automated proof search and this can be
used to discover long sequences of first-order deduction steps in a proof.  
Consequently, mathematicians can spend less time carrying out the parts of an 
argument that are more-or-less obvious (once discovered), and more time 
contemplating deeper questions.
% Automation is especially useful for highly computational proofs with hundreds or
% thousands of cases.


\subsection{The usability gap}
Despite the many advantages and the noteworthy success stories mentioned above, proof assistants remain relatively obscure. There are a number of obvious reasons for this.  First and foremost, proof assistant software tends to be tedious to use. Most mathematicians experience a significant slow down in progress when they must not only formalize every aspect of their arguments, but also express such formalizations in a language that the software is able to parse and comprehend.

One question that leads to insight into what we call the ``usability gap'' that plagues most modern proof assistants is why \emph{computer algebra systems} do not suffer as much from the same problem. Simply put, computer algebra systems are more popular than interactive theorem provers. One reason is that the up-front cost to end users seems substantially higher for an interactive theorem prover than for a computer algebra system, and this is likely because the latter is typically conceived of by a mathematician whose primary aim is to build a system that presents things ``as they should be,'' that is, as a mathematically educated user would expect. To a mathematician, such systems are \emph{well-designed}. 

In many theorem proving systems, things are often not presented as we would expect or like them to be. In~\cite{PolletKerber:2007}, Pollet and Kerber argue that this is not just a deficiency of the user interface. The problem with theorem provers goes much deeper; it goes to the core of these systems, namely to \emph{the formal representation of mathematical concepts and knowledge}. How easy or hard it is to codify theorems and translate informal mathematical arguments into formal proofs in a particular system depends crucially on the 
formal foundations of that system, and the way in which these foundations are represented in the system.

\subsection{Closing the usibility gap}
Our research program addresses a few of the major roadblocks to wide-spread adoption of proof assistants; specifically,

\begin{itemize}
\item \emph{Incomplete mathematical libraries.} Formal libraries---consisting of  definitions, lemmas, and proofs---are a prerequisite for most formal developments. Before users can apply proof assistants to their own research, they need to formalize a lot of undergraduate- and graduate-level mathematics. Even the largest formal libraries currently available cover only a small fraction of mathematics.

\item \emph{Weak automation of mathematics.} Proof assistants typically combine general-purpose logical automation and procedures for arithmetic. But without \textsl{domain-specific automation} (\dsa), a single sentence in a proof in the Informal Language can correspond to dozens or hundreds of lines in a formal language.

\item \emph{Poor interoperability with computer algebra systems.} Notwithstanding the
  existence of a few prototypes~\cite{MR1730396,MR1656868}, interoperability between
  computer algebra systems and proof assistants is an open problem. A trustworthy
  integration of algebra systems in proof assistants requires the validation of
  certificates produced by the algebra systems. However, most procedures do not generate
  certificates; for some algorithms, it is not even clear what certificates would look
  like [24]. 
  % Conversely, users of algebra systems could exploit the proof assistants'
  % support for expressing and finding proofs.

\item \emph{Steep automation learning curve.}  Extending a proof
  assistant to support \textsl{domain-specific automation} requires a high level
  of expertise. The programmatic interfaces of the main proof assistants have
  evolved over several decades and are difficult to learn. For example,
  to extend Coq, one must learn both the Ltac tactic language and the
  low-level OCaml interfaces, in addition to the Gallina specification language.
\end{itemize}

  Despite the many obstacles, there is a strong
  feeling in various parts of the research community that mathematics deserves to
  be formalized. Fields medalist Vladimir Voevodsky was one of
  the strongest advocates of this view. His research area was plagued by flawed theorems,
  to the point where he stopped believing paper-and-pencil proofs~\cite{rehmeyer:2013}.


The overriding goal of our project is to re-examine and formalize the foundations of 
mathematics---with a particular focus on our primary areas of expertise, universal algebra---to do so in a \emph{practical} and \emph{computable} way, and to codify these foundations and advance the state-of-the-art in computer-aided theorem proving technology. The goal will be achieved when the software becomes a natural, if not necessary, part of the working mathematician's toolbox.  We envision a future in which we can hardly imagine proving new theorems, completing referee assignments, or communicating and disseminating new mathematics without the support of a proof assistant.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Universal Algebra and its Role in the Project}
\emph{Universal} (or \emph{general}) \emph{algebra} has been invigorated in recent years by a small but growing community of researchers exploring the connections between algebra and computer science. Some of these connections were discovered only recently and were quite unexpected. Indeed, algebraic theories developed over the last 30 years have found a number of important applications in both of the two main branches of theoretical computer science---\emph{Theory~A}, comprising \emph{algorithms} and \emph{computational complexity}, and \emph{Theory~B}, comprising
\emph{domain theory}, \emph{semantics}, and \emph{type theory} (the theory of programming 
languages).\footnote{In the sequel, we use the fairly standard acronym \tcs for \emph{theoretical                          computer  science}, broadly construed.
                    The Theory~A--Theory~B dichotomy was established by ``The Handbook of Theoretical Computer Science''~\cite{vanLeeuwen:1991A,vanLeeuwen:1991B}, Volume~A of which includes chapters on algorithms and complexity theory; Volume~B covers domain theory, semantics, and type theory.}
The present proposal falls within the scope of Theory B.

\subsection{Foundations of mathematics and computing science}
Universal algebra, lattice theory, and category theory have had a deep and lasting impact on the development of theoretical computer science,  particularly the subfields of domain theory, denotational semantics, and programming languages research~\cite{MR1249550}. Dually, progress in theoretical computer science has informed and inspired a substantial amount of pure mathematics in the last half-century~\cite{MR3662915,MR3725758,MR2765040,MR3233442,MR1321662, MR1249550}, %MR2855321,MR1344959,MR2525959, 
just as physics and physical intuition motivated so many mathematical discoveries of the last two centuries.

% The mathematical foundations of theoretical computer science are essentially the same as the mathematical foundations of mathematics. Indeed, much of the pure mathematics literature can be classified as \tcs research; similarly, many \tcs papers could easily fall under mathematics or logic research. (For examples, see~\cite{MR3662915,MR3725758,MR2765040,MR2855321,MR3233442,MR2525959, MR1344959,MR1321662, MR1249550}.)

The availability and accessibility of \emph{functional} programming languages supporting \emph{dependent types} has given rise to new opportunities to apply abstract concepts from universal algebra and category theory to the practice of programming, to yield code that is more modular, reusable, and safer, and also to express programming constructs that would be incomprehensible or inconceivable in an \emph{imperative} or \emph{procedural} programming language. 
(For examples, see~\cite{baueroplss:2018},~\cite[Chs. 5 \& 10]{chiusano:2014}.) These are exciting developments for the design and implementation of computer programs, and this has some relevance to our work.  However, our focus is on the development and codification of the foundations of general algebra for research in pure mathematics.
 
In the remainder of this project description, we give some background on interactive theorem proving technology, introduce dependent type theory, and describe the \emph{Lean} proof assistant. We will demonstrate how and why these technologies are well suited to research in universal algebra. Thereafter we present the concrete goals of our research program and explain how we intend to accomplish them. Along the way, we will see some examples of the achievements we have already made in pursuit of these goals.

Before proceeding, however, let us summarize in broad terms and using nontechnical language the main objectives of this research project.
\begin{enumerate}[{\bf 1.}]
  \item To present the core of universal algebra using \emph{practical} logical foundations, which means that, when possible, definitions, theorems and proofs should be constructive and have computational meaning.
  
  \item To develop software that extends the \emph{Lean Mathematics Library}~\cite{lean-mathlib:2018} to include the output of task (1), \emph{implementing} the core results of our field as \emph{types} and their proofs as \emph{programs} (or \emph{proof objects}) in Lean's \emph{dependent type theory}.
        
  \item To develop \emph{domain-specific automation (\dsa)} tools that help working mathematicians harness the power of modern proof assistant technology.
        
  \item To teach mathematicians how to use the assets developed in items (1)--(3)
        to do the following:
  \begin{enumerate}[{\bf a.}]
    \item translate existing or proposed Informal Language proofs (typeset in \LaTeX, say) into Lean so they can be formally verified and tagged with a certificate of correctness;
    \item construct and formally verify proofs of new theorems using Lean;
    \item import (into Lean) software packages and algorithms used by algebraists 
    (e.g. UACalc or GAP) so that these tools can be certified and subsequently 
    invoked when constructing formal proofs of new results.
  \end{enumerate}
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% [49] R. Nederpelt, H. Geuvers, and R. de Vrijer, editors. Selected Papers on Automath, volume
%% 133 of Studies in Logic and the Foundations of Mathematics. Elsevier Science, 1994.



\section{The Lean Theorem Prover and its Role in the Project}
Given our motivation, the choice of language and proof system to use was easy; we chose the \emph{Lean proof assistant} because it is designed and developed by logicians and computer scientists working together to create a language and syntax that presents things \emph{as they should be}, so that the working in the language feels almost as natural as working in the Informal Language and is easily adopted by mathematicians who lack special training in computer science. 

\emph{Lean} is a relatively new programming language and proof assistant developed at Microsoft Research and Carnegie Mellon University. Lean draws on decades of experience in interactive and automatic theorem provers (e.g., Coq, Isabelle/HOL, and Z3). Its logic is very expressive, and emphasis is placed on \emph{powerful proof automation}. The system is easy to extend via \emph{metaprograms} developed in the same logical language as is used to express specifications and proofs in Lean. In this way, Lean aims to \emph{bridge the gap between interactive and automated theorem proving}.

There are many other reasons Lean is an ideal platform for this project.
For instance, it is unique among computer-based theorem proving tools in that its
\emph{proofs tend to be easy to read and understood} without special training. 
In fact, working in Lean usually leads to formal proofs that are cleaner, more concise, 
and shorter than the corresponding proofs in the Informal Language.

To support the formalization of theorems, we will develop libraries that contain formal statements and proofs of all of the core definitions and results universal algebra, and we will explore how to automate proof search specifically in these foundational areas.  We will also formalize theorems emerging from our own mathematics research. We are currently engaged in three other research projects, two in universal algebra and one in lattice theory.  A proof assistant equipped with special libraries of definitions and results from algebra and lattice theory, as well as specialized tactics to automate the standard \emph{proof idioms} of our field, would be extremely useful for our other research projects. One of our goals is to demonstrate (to ourselves and to our colleagues around the globe) the utility of such libraries and tactics for proving ``real world'' theorems.

We will develop techniques and tools that help mathematicians perform accurate computations and deductive reasoning using proof assistants, integrating procedures from computer algebra systems in a verified way. Finally, we will contribute to Lean's development with ideas and features designed to benefit all users, notably an \emph{efficient built-in procedure for first-order reasoning and an integration with automatic theorem provers}. 

Lean is a very young language, and its domain-specific libraries for special disciplines are small but growing. \emph{It is vital for mathematicians to get involved at this early stage and play a leading  role in its development.}  If we leave all of this to our colleagues in computer science, they will base the development on their perception of our needs, history will likely repeat itself, and 
it is unlikely that the libraries and tools that come out of the effort will meet the needs and expectations of most working mathematicians.

\subsection{Lean's Hierarchy of Sorts and Types}
Like its more mature cousins Agda and Coq, Lean takes for its logical foundations \emph{dependent type theory} with \emph{inductive types} and \emph{universes}. However, unlike Agda and Coq, Lean's universes are \emph{not cumulative}.\footnote{Although we are trying hard to avoid overly technical discussions here, we will point out that lack of cumulative universes in Lean does not reduce Lean's expressiveness; instances in which universe cumulativity would be exploited in Coq can be handled in Lean using \emph{universe polymorphism} and the \lstinline{lift} map.}
At the bottom of the hierarchy is \lstinline{Type 0}, which is usually denoted by \lstinline{Type}.
Think of  \lstinline{Type} as the universe of ``small'' or ``ordinary'' types.  At the next level is \lstinline{Type 1}, which is a larger universe of types that contains \lstinline{Type} as an \emph{element}. \lstinline{Type 2} is  larger still and contains  \lstinline{Type 1} as an element, and so on. 

The upshot of this \emph{ramified system} is that all of the types described in the last paragraph are \emph{predicative}, which means that their definitions are not self-referential, so certain set-theoretic paradoxes (e.g. Russel's) are avoided. However, certain situations call for a self-referential type, so Lean supplies one. It is the \emph{impredicative} type \lstinline{Prop}.
A synonym for \lstinline{Prop} is \lstinline{Sort}, which is shorthand for \lstinline{Sort 0}. The type of \lstinline{Prop} (and \lstinline{Sort} and  \lstinline{Sort 0}) is \lstinline{Type 0}; that is, \lstinline{Prop : Type 0}. In general, for all \lstinline{n ≧ 0}, 
% we have \lstinline{Sort n : Type n} and More generally, for each natural number \lstinline{n}, 
we have 
\lstinline{Sort n+1 = Type n}, whose type is \lstinline{Type n+1}. 
To summarize, we have the following type identities and relations:
\begin{lstlisting}
            Sort 0 = Prop               Prop : Type 0
            Sort 1 = Type 0           Type 0 : Type 1
            Sort 2 = Type 1           Type 1 : Type 2
                   ⋮                         ⋮
          Sort n+1 = Type n           Type n : Type n+1
\end{lstlisting}
See Section~\ref{sec:sorts-and-types} of the appendix for some examples 
demonstrating Lean's ramified hierarchy of sorts and types.

We also want some operations to be polymorphic over type universes. For example,  \lstinline{list α} should make sense for any type  \lstinline{α}, no matter which universe  \lstinline{α} lives in. Thus \lstinline{list : Type u_1 → Type u_1}, where \lstinline{u_1} is a variable ranging type levels. 

One aspect that makes the Lean kernel trustworthy is its size; at its core, it implements a very small set of components, namely,
\begin{itemize}
  \item \emph{dependent lambda calculus};
  \item \emph{universe polymorphism} in a hierarchy of $\omega$-many universe levels;
  \item \emph{inductive types} and \emph{inductive families of types}, generating only the recursor for an inductive type;
  \item \emph{pattern matching}.
\end{itemize}

% Lean's core kernel does \emph{not} implement the following:
% \begin{itemize}
% \item a termination checker;
% \item fixpoint operators.
% \end{itemize}
% Lean supports \emph{inductive families} and \emph{mutually defined inductive types},
% but not \emph{induction-induction} or \emph{induction-recursion}.

Lean is easy to extend via \emph{metaprogramming}. Briefly, a \emph{metaprogram} is
a program whose purpose is to modify the behavior of other programs. \emph{Proof tactics} form an important class of metaprograms. These are automated procedures for constructing and manipulating proof terms. A distinguishing feature of Lean is that \emph{metaprograms can be written in the Lean language}, rather that in the lower level language (C/C++) that was used to create Lean. Thus the metaprogramming language is the same logical language that we use to express specifications, propositions, and proofs.



% As mentioned in the introduction, a \emph{proof assistant}
% is an interactive tool that makes it possible to develop machine-checked formal 
% proofs of theorems, and to automate the construction of proofs.
% Two early systems were Nicholas de Bruijn's \emph{Automath}~\cite{nederpelt:1994} and
% Andrzej Trybulec's \emph{Mizar}~\cite{MR3806496}.
% Both of these defined a formal language for writing mathematical definitions and proofs, 
% and for expressing complete mathematical theories in such a way that an automated proof checker 
% could mechanically verify their correctness.
% Mizar also includes a library of formalized mathematics that can be used to prove new
% theorems.\footnote{As of 2009, the \emph{Mizar Mathematical Library} was 
% the largest coherent body of strictly formalized mathematics in existence.~\cite{Wiedijk:2009}}
% Some recent incarnations of such systems are Agda~\cite{Norell:2009}, 
% Coq~\cite{CoqManual:2017}, Lean~\cite{nuprl:1979}.  Some famous examples of
% formalizations acheived with these systems include the formal proofs of the \emph{Kepler Conjecture} by Tom Hales, et al.~\cite{MR3659768},
% and the \emph{Feit-Thompson Odd-order Theorem} by Georges Gonthier, et 
% al~\cite{gonthier:2013b}.\footnote{Remarkably, Hales's first attempt at publishing his 
% highly complex proof of the Kepler conjecture failed after many years and countless 
% man-hours were spent by referees attempting (unsuccessfully) to verify the informal proof.
% It was after this experience that Hales formalized and verified his result with a proof assistant.}
%% After this failure to have his result accepted, a defiant Hales
%% responded by formalizing his entire proof.

% Each of the systems mentioned above are based on different logical foundations.
% For those based on type theory, the particular interpretation of type theory 
% differs from one system to the next.
% While many are based off Per Martin-L\"of's ideas, most have added features, more axioms,
% or a different philosophical viewpoint.
% For instance, the \nuprl system is based on \emph{computational type theory}, while Coq and Lean are 
% based on the \emph{calculus of (co)inductive constructions} (\cic).
% 
% \subsection{Martin-L\"of type theories}
% Per Martin-L\"of constructed several type theories that were published at various times, some of them much
% later than the preprints with their description became accessible to the specialists.
% All of the theories had \emph{dependent products}, \emph{dependent sums}, \emph{disjoint unions},
% \emph{finite types} and \emph{natural numbers}.
% All the theories had the same reduction rules that did not include η-reduction
% either for dependent products or for dependent sums (except for the language MLTT79
% where the η-reduction for dependent products was added).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Dependent Types}
Lean is a functional programming language that supports \emph{dependent types}.
Here we explain what dependent types are.  Later, when we present some 
examples of constructions in universal algebra, we will see exactly how dependent types support precise representations of many of the basic objects of interest in universal algebra.
Besides being more precise and elegant that other representations, dependent types are constructive, and (provided we work in a language that supports them) we can compute them.

What makes dependent types \emph{dependent}? % and why dependent types are useful. 
The short explanation is that types can depend on \emph{parameters}. For example,the type \lstinline{list α} depends on the argument \lstinline{α}, and this dependence is what distinguishes \lstinline{list ℕ} from \lstinline{list bool}. Thus, dependent types are \emph{polymorphic}.  However, dependent types differ from so-called \emph{generic types} (which depend on \emph{type parameters}).  Let us demonstrate with another example.  
Consider the type \lstinline{vec α n}, that is, the type of vectors of length 
\lstinline{n} whose entries inhabit the type \lstinline{α}. The type \lstinline{vec α n} depends on one type parameter, \lstinline{α : Type}, and one \emph{value parameter}, \lstinline{n : ℕ}.  Thus, dependent types are generic types that can depend, not only on type parameters, but also on value parameters.

Suppose we wish to write a function \lstinline{cons} that inserts a new element at the head of a list. What type should \lstinline{cons} have? Such a function is polymorphic: we expect the \lstinline{cons} function for \lstinline{ℕ}, \lstinline{bool}, or an arbitrary type \lstinline{α} to behave the same way. Thus, it makes sense to take this type to be the first argument to \lstinline{cons}, so that for any type \lstinline{α}, \lstinline{cons α} is the function that takes an element \lstinline{a : α} and a list 
\lstinline{ℓ : list α} and returns \lstinline{cons α a ℓ = a :: ℓ} 
(that is, \lstinline{ℓ} with \lstinline{a} prepended).

While it's clear that \lstinline{cons α} should have type \lstinline{α → list α → list α}, less clear is what type \lstinline{cons} should have; perhaps \lstinline{Type → α → list α → list α}. But this is nonsense since the type parameter \lstinline{α} does not refer to anything, whereas it should refer to whatever is passed in for the argument of type \lstinline{Type}. In other words, \emph{assuming} \lstinline{α : Type} is the first argument to \lstinline{cons}, then the type of the next two arguments will be \lstinline{α} and \lstinline{list α}, which are types that depend on the first input argument. 

This example is an instance of a \emph{dependent function type}, also known as a \lstinline{Pi} type. Given \lstinline{α : Type} and \lstinline{β : α → Type},
view \lstinline{β} as an indexed family of types over the index set \lstinline{α}; for each \lstinline{a : α} we have a type \lstinline{β a}.  The type \lstinline{Π(x : α), β x} denotes the type of functions \lstinline{f} with the property that, for each \lstinline{a : α, f a} is an element of \lstinline{β a}, so the type of the value returned by \lstinline{f} \emph{depends} on the input argument. In case \lstinline{β} is a constant function, the type \lstinline{Π(x : α), β x} is no different from the (non-dependent) function type \lstinline{α → β}. Indeed, in dependent type theory, and in Lean, the \lstinline{Pi} construction is fundamental, and \lstinline{α → β} is just notation for \lstinline{Π(x : α), β x} in the special case in which \lstinline{β} is constant.

% \subsection{Curry-Howard Correspondence}
% % Propositions-as-types, proofs-as-programs}
% % \textsl{Types are propositions.}
% The rule for \emph{function application} corresponds, under the 
% ``Curry-Howard'' or ``propositions-as-types'' correspondence, to the \emph{implication elimination} rule of 
% natural deduction (sometimes called \emph{modus ponens}).  It is the following:
% \begin{prooftree}
%   \AxiomC{$f \colon A \to B$}
%   \AxiomC{$a \colon A$}
%  \RightLabel{$_{\mathsf{app}}$}
%   \BinaryInfC{$f \, a \colon B$}
% \end{prooftree}
% This simply codifies our intuitive notion of function application, viz.~applying the 
% function $f$ to an inhabitant $a$ of the domain $A$, we obtain an inhabitant $f \, a$
% of the codomain $B$.  If we interpret $A$ and $B$ as propositions,
% $f$ as a proof of the implication $A \to B$, and  
% $a$ as a proof of $A$, then the rule $\mathsf{app}$ becomes the implication elimination rule (\emph{modus ponens}).
% % 
% % \textsl{Programs are proofs.}




\subsection{Lean's Elaboration Engine}

On top of the Lean kernel there is a powerful \emph{elaboration engine} that can
\begin{enumerate}
\item infer implicit universe variables,
\item infer implicit arguments, using higher order unification,
\item support overloaded notation or declarations,
\item insert coercions,
\item infer implicit arguments using type classes,
\item convert readable proofs to proof terms,
\item construct terms using tactics.
\end{enumerate}
Lean does most of these things simultaneously. For instance, the term constructed by
type classes can be used to find out implicit arguments for functions.
A nice list of examples demonstrating of each of these key features of Lean can be found in the blog post by Floris van Doorn available at
\url{https://homotopytypetheory.org/2015/12/02/the-proof-assistant-lean/}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Domain Specific Automation}
One of the primary objectives of Lean is to bridge the gap between automatic and interactive theorem proving. A cornerstone for this vision is \emph{white-box automation:} the key techniques underlying SMT solvers, such as the \emph{congruence closure} for reasoning about equality~\cite{MR3536762} and the quantifier instantiation heuristics~\cite{MR2458080}, are available to users via the metaprogramming framework. Despite this, several reasoning components, as well as some important metaprogramming ``tape and glue,'' are missing. For all its potential, Lean is not as automated as Isabelle/HOL.

We will design a cohesive set of reasoning components. As a young system, Lean is an ideal test-bed for new ideas. A strong reconstruction tactic based on a procedure for first- or even higher-order logic is critical to automation. The existing super tactic [30] is unfortunately orders of magnitude too slow to be useful. Developing a monadic prover with efficient data structures and finding solutions to speed up the execution of all metaprograms will require innovative ideas.

We also want to contribute to the Lean simplifier’s support for arithmetic and the envisioned algebraic normalizer, based on our experience working towards the first two objectives. The main difficulty is to balance raw deductive power, extensibility, and predictability. Furthermore, we will design a relevance filter that selects lemmas from background libraries, for use with tactics. One
of the challenges is to perform the heuristic selection in a robust way, so that filtering can be used both for proof search and for rechecking the proof. Existing filters, whether they rely on machine learning [12] or not, suffer from a lack of ``diversity:'' they tend to select many equivalent lemmas
while ignoring some key lemmas. Ideas from research in search engines will likely apply.

Finally, it will be useful to integrate external automatic theorem provers in Lean. A new generation of higher-order provers is emerging—including Leo-III, Satallax, and higher-order versions of CVC4, E, Vampire, and veriT. My ERC project Matryoshka 3 funds work on E and veriT. Integrating these provers will be a high-impact deliverable that will dramatically increase automation.
The challenge is to design a sound, complete, and yet efficient encoding of Lean’s rich type system, based on my work on polymorphic type encodings [11]—and to reconstruct proofs in Lean.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Proof of Concept: basic universal algebra in Lean}
% This section outlines a few aspects of our plan to examine the core foundations of our main area of expertise, universal algebra, and represent these foundations in a way that is not only easy to codify in Lean, but also remains accessible to working mathematicians. The resulting libraries will aim to make it easier to use Lean to find new proofs, verify existing proofs, and test conjectures.

% Specifically, we 
This section explains how and why dependent and inductive types 
are useful for efficiently expressing certain fundamental constructs of universal algebra.
A few concrete examples will showcase the power and expressiveness of the Lean language,
and demonstrate how it enables us to codify advanced mathematics in a constructive and computable way.  In particular, we will show how to use Lean to represent each of the following concepts: 
\emph{operation}, \emph{algebra}, \emph{subuniverse}, and \emph{term algebra}.
Our formal representations of these will be clear, concise, and computable, and we will develop a notation and formal syntax that seems natural and self-explanatory to a general algebraist.
Our goal is to demonstrate the power of Lean's type system for expressing 
mathematical concepts constructively, and to show that if we make
careful design choices at the start of our development, then our formal 
theorems and proofs can be as efficient and readable as their Informal Language analogs.

\subsection{Operations and Algebras}
The symbols $\N$, $\omega$, and {\tt nat} are used interchangeably; they all denote the 
set of natural numbers. A \emph{signature} $S = (F, \rho)$ consists of a set $F$ of 
\emph{operation symbols}, along with a \emph{similarity type} function $\rho \colon F \to N$. 
The value $\rho f \in N$  is called the \emph{arity} of $f$. Although in the classical theory 
we usually assume $N = \N$, for much of the basic theory this is inconsequential and, as 
we will see below, when implementing general operations in Lean
it is often unnecessary to commit in advance to a specific 
\emph{arity type}.\footnote{An exception is the quotient algebra construction
since, unless we restrict ourselves to finitary operations, lifting a basic operation 
to a quotient= requires some form of choice.}

Classical universal algebra is the study of \emph{varieties} (or \emph{equational classes}) 
of algebraic structures where an \emph{algebraic structure} is denoted by $\alg A = \< A, F^{\alg A}\>$ and consists of a set $A$, called the \emph{carrier} of the algebra, along with a set $F^{\alg A}$ of operations defined on $A$, one for each operation symbol; that is,
\[F^{\alg A} = \{f^{\alg A} \mid f\in F \text{ and } f^{\alg A} \colon (\rho f \to A) \to A\}.\] 

Some of the renewed interest in universal algebra has focused on representations of algebras 
in categories other than $\set$, multisorted algebras, and higher type universal algebra.  
(See, for example,~\cite{MR2757312,MR3003214,finster:2018,gepner:2018,MR1173632}.)  These are natural generalizations that we plan to integrate into future iterations of the 
\lstinline{lean-ualib}, once we have a working implementation of the core of classical (single-sorted, set-based) universal algebra.

Suppose $A$ is a set and $f$ is a $\rho f$-ary 
operation on $A$. In this case, we often write $f \colon A^{\rho f} \to A$.  
If $N$ happens to be $\N$, then $\rho f$ 
denotes the set $\{0, 1, \dots, \rho f -1\}$ and a function $g \colon \rho f \to A$, identified with its graph, is simply a $\rho f$-tuple of elements from $A$.
% ; that is, for each $i\in \rho f$, $g i \in A$.
By identifying $A^{\rho f}$ with the type $\rho f \to A$ of functions from $\rho f$ to $A$, 
we identify the $A^{\rho f} \to A$ with the type 
$(\rho f \to A) \to A$. 
The notation $f \colon A^{\rho f} \to A$ is then equivalent to the assertion that $f$ inhabits
the type $(\rho f \to A) \to A$. 
% , denoted $f \colon (\rho f \to A) \to A$.

Fix $m\in \N$. An $m$-tuple, $a = (a_0, a_1, \dots, a_{m-1}) \in A^m$ is 
literally a function $a \colon m \to A$, defined for each $i < m$ by $a\, i = a_i$.
Therefore, if $h \colon A \to B$, then $h\circ a \colon m \to B$ is the tuple 
% $(h \, a_0, h \, a_1, \dots, h \, a_{m-1})\in B^m$, 
whose value at $i$ is $(h\circ a)\, i = h \, a\, i = h\, a_i$ and has type $B$. 
On the other hand, if $g \colon A^m \to A$, then $g \, a$ has type $A$.
% is the element $g(a_0, a_1, \dots, a_{m-1}) \in A$.
If $f \colon (\rho f \to B) \to B$ is a $\rho f$-ary operation on $B$,  
if $a \colon \rho f \to A$ is a $\rho f$-tuple on $A$, and if 
$h \colon A \to B$, then $h \circ a \colon \rho f \to B$, so 
$f (h \circ a) \colon B$.




\subsection{Operations and Algebras in Lean \href{https://github.com/UniversalAlgebra/lean-ualib/blob/master/src/basic.lean}{(\lstinline{lean-ualib/basic.lean})}}\

\noindent Let us now look at the actual Lean implementation of a few basic concepts from
universal algebra, highlighting the similarity that can be achieved between the formal and 
informal rendering of these concepts.
We start with the \emph{type of operation symbols} and the \emph{type of signatures}.
% import data.set
\begin{lstlisting}
  import data.set
  definition op (β α) := (β → α) → α
\end{lstlisting}  
An example of an operation of type \lstinline{op} is the projection function \lstinline{π},
of arity \lstinline{β} on the \emph{carrier type} \lstinline{α}, which we define in Lean as follows:
\begin{lstlisting}
  definition π {β α} (i) : op β α := λ a, a i
\end{lstlisting}
The operation \lstinline{π i} maps a given tuple \lstinline{a : β → α} to its value \lstinline{a i} at  \lstinline{i}. For instance, suppose we have types $\alpha$ and $\beta$, and 
variables \lstinline{i : β} and \lstinline{f : β → α},
\begin{lstlisting}
  variables (α : Type*) (β : Type*) (i : β) (f : β → α) 
\end{lstlisting}
Then the command \lstinline{#check π i f} shows that the type of \lstinline{π i f} is \lstinline{α}, as expected, since \lstinline{π i f = f i}.

We define a signature as a structure with two fields, the type \lstinline{F} of operation symbols and an \emph{arity function}
\lstinline{ρ : F → Type*}, which maps each operation symbol $f$ to its arity $\rho f$.
\begin{lstlisting}
  structure signature := mk :: (F : Type*) (ρ : F → Type*)
\end{lstlisting}  
Next we define the \emph{type of interpretations of operations} on the carrier type \lstinline{α}.
First, let us fix a signature \lstinline{S} and define some convenient notation.\footnote{The  \lstinline{section} command allows us to open a section throughout which our signature \lstinline{S} will be available.  The \lstinline{section} ends when the keyword \lstinline{end} appears below.}
\begin{lstlisting}
  section 
    parameter {S : signature}
    definition F := S.F
    definition ρ := S.ρ 
    definition algebra_on (α : Type*) := Π (f : F), op (ρ f) α   
    -- (section continued at * below)
  \end{lstlisting}  
The first definition allows us to write \lstinline{f : F} (instead of \lstinline{f : S.F}) 
to indicate that the operation symbol \lstinline{f} inhabits \lstinline{F}; 
the second definition is included so that, instead of \lstinline{S.ρ f}, we can denote the arity of \lstinline{f} by \lstinline{ρ f}, which is identical to the familiar Informal Language syntax.

The definition of \lstinline{algebra_on} makes sense, if we are given a signature \lstinline{S} and a carrier type \lstinline{α}, then an \lstinline{S}-algebra over \lstinline{α} is determined by its operations on $\alpha$.\footnote{plus whatever equational laws it models.} 
An inhabitant of the type \lstinline{algebra_on} assigns an interpretation to each \lstinline{op} symbol \lstinline{f : F}, which yields a function of type \lstinline{(ρ f → α) → α}.

Finally, we define an algebra.  Since an algebra pairs a carrier with an interpretation of the operation symbols, we use the \emph{dependent pair type}, 
\lstinline{Σ (x : A), B x}, also known as 
a \emph{Sigma type}. This is the type of ordered pairs \lstinline{<a, b>}, where \lstinline{a : A} and \lstinline{b} is of type \lstinline{B a}, which may depend on \lstinline{a}. Just as a \emph{Pi type} \lstinline{Π (x : A), B x} generalizes the notion of function type \lstinline{A → B} by allowing the codomain \lstinline{B x} to depend on the input argument \lstinline{x}, a Sigma type \lstinline{Σ (x : A), B x} generalizes the cartesian product \lstinline{A × B} by allowing the type \lstinline{B x} of the second argument of the ordered pair to depend on the first, \lstinline{x}.\footnote{Lean's built-in \lstinline{sigma} type is defined as follows:\\
\lstinline{structure sigma {α : Type u} (β : α → Type v) := mk :: (fst : α) (snd : β fst)}}
  
Since an algebra $\<A, F^{\alg A}\>$ is an ordered pair where the type of the second argument depends on the first, it is natural to encode an algebra in type theory using a Sigma type. 
Indeed, we define \lstinline{algebra} in \lstinline{lean-ualib} as follows:
\begin{lstlisting}
  -- (section continued from * above)
  definition algebra := sigma algebra_on
    instance alg_carrier : has_coe_to_sort algebra := ⟨_, sigma.fst⟩
    instance alg_operations : has_coe_to_fun algebra := ⟨_, sigma.snd⟩
  end
\end{lstlisting}  
The last two lines are tagged with \lstinline{has_coe_to_sort} and \lstinline{has_coe_to_fun}, respectively, which allows us to take advantage of a very nice feature of Lean called \emph{coercions}. Using this feature we can write programs using syntax that looks very similar to our Informal Language. For instance, the standard notation for the interpretation of the operation symbol $f$ in the algebra $\alg A = \<A, F^{\alg A}\>$ is $f^{\alg A}$.  In our implementation, the interpretation of $f$ is denoted simply by \lstinline{A f}. While \lstinline{A f} is not identical to
the Informal Language's $f^{\alg A}$, one could argue that it is just as elegant, and that adapting to it does not impose an unacceptable burden on the user. 
% Another example that demonstrates the utility of coercions is our definition of \lstinline{is_subalgebra}, a function that takes as input two algebraic structures and decides whether the second structure is a subalgebra of the first.  Here is the definition.
% \begin{lstlisting}
%     def is_subalgebra {S : signature} {α : Type*} {β : Type*}
%     (A : algebra_on S α) {β : set α} (B : algebra_on S β) :=
%     ∀ f b, ↑(B f b) = A f ↑b
%   \end{lstlisting}
To see this notation in action, let us look at how the \lstinline{lean-ualib} represents the assertion that a function is an \lstinline{A}-homomorphism.
\begin{lstlisting}
  definition homomorphic {S : signature} 
  {A : algebra S} {B : algebra S} (h : A → B) := 
  ∀ f a, h (A f a) = B f (h ∘ a)
\end{lstlisting}
If we compare this with a common Informal Language representation of homomorphism, say,
$\forall f \; \forall a \; h (f^{\alg A} (a)) = f^{\alg B} (h \circ a)$,
we see that the \lstinline{lean-ualib} definition is quite similar.







%|||||||||||||||||||||||||||| OMIT BELOW |||||||||||||||||||||||||||||||||||||||
%VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV
\ifextver

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% General Projection and Composition %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{General Projection and Composition}
Suppose $f \colon (\rho f \to A) \to A$, and suppose 
$g_i \colon A^m \to A$ for each $i <\rho f$.
Let $g \colon \rho f \to (A^m \to A)$ denote
the function whose value at $i < \rho f$ is 
$g\, i = g_i$. We want to define a \emph{generalized composition} of $f$ 
with $g_0, g_1, \dots, g_{\rho f -1}$.  We could obviously do this component-wise,
and specify the value of $f \circ g$ component-wise,
but that makes computing with such compositions unweildy: for all $a : m \to A$, we define
\[
  f [g](a) := f[g_0, \dots, g_{\rho f -1}](a) := f (g_0 a, \dots, g_{\rho f-1} a).
  \]

Now, if  $\mathsf{app}$ denotes the formal \emph{function application rule}, then
\begin{prooftree}
  \AxiomC{$f \colon (\rho f \to A) \to A$}
  \AxiomC{$a \colon \rho f \to A$}
  \RightLabel{$_{(\mathsf{app})}$}
  \BinaryInfC{$f a \colon A$}
\end{prooftree}
\begin{prooftree}
  \AxiomC{$g \colon \rho f \to ((m \to A) \to A)$}
  \AxiomC{$i \colon \rho f$}
  \RightLabel{$_{(\mathsf{app})}$}
  \BinaryInfC{$g i \colon (m \to A) \to A$}
  \AxiomC{$b \colon m \to A$}
   \RightLabel{$_{(\mathsf{app})}$}
  \BinaryInfC{$g i b\colon A$}
\end{prooftree}
Apparently composition of $f$ with $g$ is impossible without dropping down to coordinates
since the types don't line up properly. 
However, this is easily fixed with an obvious isomorphism. 
Denote by $\uncurry{g} \colon (\rho f \times (m \to A)) \to A$
the uncurried version of $g$, so that $gib = \uncurry{g}(i,b)$.
Swapping the first and second coordinates of $\uncurry{g}$ yields
$\swap{\uncurry{g}} \colon ((m\to A) \times \rho f) \to A$; that is  
$\swap{\uncurry{g}}(b,i) = \uncurry{g} (i,b)$ for all $i \colon \rho f$ and $b \colon m \to A$.  
Now, if we let $\tilde{g} := \curry{\swap{\uncurry{g}}}$, then the types of $f$ and $\tilde{g}$ 
are properly aligned for composition.
Indeed, we have
\begin{prooftree}
  \AxiomC{$f \colon (\rho f \to A) \to A$}
  \AxiomC{$\tilde{g} \colon (m \to A) \to (\rho f  \to A)$}
  \AxiomC{$b \colon m \to A$}
  \RightLabel{$_{(\mathsf{app})}$}
  \BinaryInfC{$\tilde{g} b \colon \rho f \to A$}
  \RightLabel{$_{(\mathsf{app})}$}
  \BinaryInfC{$f \tilde{g} b \colon A$}
\end{prooftree}
and for each $b \colon m \to A$, the function $\tilde{g}b \colon \rho f \to A$ is the tuple 
whose $i$-th coordinate is $\tilde{g}b(i) = g_i(b_0, \dots, b_{m-1})$. Thus,
\[
  f\tilde{g} b = f(g_0 (b_0, \dots, b_{m-1}), \dots, g_{\rho f -1}(b_0, \dots, b_{m-1})).
  \]
This is called \defn{generalized composition} of $f : (\rho f \to A) \to A$ with 
$g \colon (\rho f \to A) \to (m \to A)$.
    

Let $I$ be a set and let $\sA = \{A_i \colon i \in I\}$ be
a collection of sets indexed by $I$.
Let $\alg{A} = \prod_{i\in I} A_i$ be the Cartesian product of 
the sets in $\sA$. 
View the elements of $\alg{A}$ as functions:
\begin{equation}
  \label{eq:7}
  a \in \prod_{i\in I} A_i \quad \iff
  \quad 
  \begin{cases}
    a\colon I \rightarrow \bigcup_{i\in I} A_i, \text{ and }&\\
    a(i) \in A_i, \text{ for each $i\in I$.} &  
  \end{cases}
\end{equation}

This correspondence simply records the fact that the product type 
(on the left of the bi-implication symbol) represents a special 
kind of function type (depicted on the right with the usual 
arrow notation for functions). In other words,~(\ref{eq:7}) says that
an element of the product $\prod_{i\in I} A_i$
is a function from $I$ into $\bigcup_{i\in I} A_i$ 
whose codomain $A_i$ \emph{depends} on the input argument $i$.  
Thus a type of the form $\prod_{i\in I} A_i$ is called a dependent function type or \emph{Pi type}.

Now, given a subset $J \subseteq I$, a function
$\sigma\colon J \rightarrow I$, and an element $a \in \prod_{i\in I}A_i$,
consider the composition $a \circ \sigma$.  This is a function from $J$ 
to $\bigcup_{j\in J} A_{\sigma(j)}$, where $(a\circ \sigma)(j) \in A_{\sigma(j)}$.
Again, we could express this function type using the arrow notation,
``$a \circ \sigma\colon J \rightarrow \bigcup_{j\in J} A_{\sigma(j)}$ 
where $(a\circ \sigma)(j) \in A_{\sigma(j)}$,'' but this 
specification has a more compact description using a Pi type:
\[
a \circ \sigma \in \prod_{j\in J} A_{\sigma(j)}.
\]
Assume $\sigma$ is one-to-one and define the ``projection'' function,
\[
\Proj \, \sigma  \colon \prod_{i\in I} A_{i} \rightarrow \prod_{j\in J} A_{\sigma(j)}
\]
by 
$\Proj \, \sigma \colon a \mapsto (a \circ \sigma)$.  That is,
$\Proj \, \sigma \, a = a \circ \sigma$. 

We could try to specify the type of $\Proj$ using the arrow notation 
as follows:
\begin{equation}
  \label{eq:8}
  \Proj \colon (J \rightarrow I)\rightarrow 
  \bigl(I \rightarrow \bigcup_{i\in I} A_{i}\bigr) \rightarrow 
  \bigl(J \rightarrow \bigcup_{i\in I} A_{i}\bigr),
\end{equation}
but the weakness of the arrow notation is now even more glaring.
The function type specification given in~(\ref{eq:8}) 
is not only imprecise, but also misleading. The result of
applying $\Proj$ first to some $\sigma \colon J \rightarrow I$ and  then
$a\colon I \rightarrow \bigcup_{i\in I} A_{i}$ is 
$\Proj \, \sigma \, \ba= \ba \circ \sigma$, and to say that this is 
a function of type $J \rightarrow \bigcup_{i\in I} A_{i}$ 
is ambiguous at best. Rather, the precise (correct!) type
specification is,
``$\Proj \, \sigma \, \ba \colon J \rightarrow \bigcup_{j\in J} A_{\sigma(j)}$ where 
$\Proj\, \sigma \, \ba \, j \in A_{\sigma(j)}$.'' Here again we can express this 
more concisely with a product type,
$\Proj \, \sigma \, \ba \in \prod_{j\in J} A_{\sigma(j)}$.
Thus, to denote the type of $\Proj$, we must add to~(\ref{eq:8}) 
the constraints on codomains that depend on argument values. 
For specifying the type of a ``function of higher order'' 
(a.k.a.~a ``functional''), the arrow notation can be cumbersome. 

The following is closer to what we want, but still imperfect:
\begin{equation}
  \label{eq:9}
  \Proj\colon (J \rightarrow I)\rightarrow \prod_{i\in I} A_{i} \rightarrow 
  \prod_{j\in J} A_{\sigma(j)}.
\end{equation}
This says that $\Proj$ takes a function 
$\sigma \colon J \rightarrow I$ and a function
$\ba \in \prod_{i\in I} A_{i}$ and returns the function 
$\ba \circ \sigma \in \prod_{j\in J} A_{\sigma(j)}$.  
Here again we see that the arrow notation is not expressive enough
because $\prod_{j\in J} A_{\sigma(j)}$ depends on $\sigma$, but no $\sigma$
is available from earlier in the expression~(\ref{eq:9}).

The solution is again to denote the function type as a Pi type.
Pi types are more expresive and allow concise specification of a dependent
product.  Before demonstrating this, we make one more notational adjustment.  
Instead of denoting set membership by $a \in A$, we will use the 
type-theoretic notation $a \colon A$, which
expresses the fact that $a$ is a constant, or nullary function, of type $A$.\footnote{Sometimes
this view of a constant as a nullary function is highlighted by denoting 
its inhabitants like this $a \colon () \to A$. Computer scientists call this device a \emph{thunk}.} 
Thus, the full (dependent) type specification of the projection operation is
\[
\Proj: \prod_{\sigma \colon J \rightarrow I}\left( \prod_{i\colon I} A_{i} \rightarrow 
\prod_{j\colon J} A_{\sigma(j)}\right).
\]
\fi
% /\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\
% | | | | | | | | | | | | | | |  OMIT ABOVE | | | | | | | | | | | | | | | | | | | | | | 







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Subalgebras %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Subuniverses}
\label{sec:subuniverses}
In this section, we describe another important concept in universal 
algebra and use it to illustrate the underlying theme that motivates
this research proposal. This is the concept of a \emph{subuniverse}.  %The concepts are \emph{subalgebras} and \emph{congruences}.

% Recall, a \emph{signature} $S = (F, \rho)$ consists of a set $F$ of \emph{operation symbols}
% along with an \emph{arity function} $\rho \colon F \to N$; that is, the \emph{arity}
% of the operation symbol $f \in F$ is $\rho f$, which belongs to some set $N$.

% An \emph{algebra} $\alg{A} = (A, F^{\alg A})$ \emph{in the signature} $S$, 
% or $S$-\emph{algebra}, is a set $A$, called the \emph{universe} of $\alg{A}$, and a set 
% $F^{\alg A} = \{ f^{\alg A} \mid f \in F\}$ of \emph{basic operations} of $\alg A$. 
% Precisely, each operation symbol $f\in F$ is given an
% \emph{iterpretation in} $\alg A$, viz.~a particular function 
% $f^{\alg A}\colon A^{\rho f} \to A$.
A \emph{subuniverse} of an algebra $\alg A = \< A, F^{\alg A}\>$ is a subset $B \subseteq A$
that is closed under the operations in $F^{\alg A}$.  
We denote by $\sansS \alg A$ the set of all subuniverses of $\alg A$.
If $B$ is a subuniverse of $\alg A$ and 
$F^{\res{\alg A}{B}} = \{\res{f^{\alg A}}{B} \mid f \in F\}$ is the set of 
basic operations of $\alg A$ restricted to $B$, 
then $\alg B = (B, F^{\res{\alg A}{B}})$ is a \emph{subalgebra} of $\alg A$.

If $\alg{A}$ is an algebra and $X  \subseteq  A$ a subset of the 
universe of $\alg{A}$, then the \emph{subuniverse of} $\alg A$ 
\emph{generated by}  $X$ is defined as follows:
\begin{equation}
  \label{eq:SgDef}
\Sg^{\alg{A}}(X)  =  \bigcap \{U \in  \sansS \alg A \mid  X  \subseteq  U\}.
\end{equation}

% \subsubsection{Congruences}
To give another exhibition of the efficiency and ease with which we can formalize basic but important mathematical concepts in Lean, we now present a fundamental theorem about 
subalgebra generation, first in the Informal Language, and then formally in Lean.
Note that the added complexity of the formal version is rather minor and superficial.

\begin{theorem}[\protect{\cite[Thm.~1.14]{MR2839398}}] 
\label{thm:1.14} Let $\alg A  = \<A,F^{\alg A}\>$  be  an  algebra in the signature 
$S = (F, \rho)$ and let $X \subseteq A$.  
Define, by recursion on $n$, the sets $X_n$ as follows:
\begin{align*}
 X_0  &=  X; \\
 X_{n+1} &=  X_n  \cup \{ f \,a  \mid f \in F, \; a \in X_n^{\rho f}\}.
\end{align*} 
Then  $\Sg^{\alg A}(X) =  \bigcup X_n$.
\end{theorem}
\begin{proof}
Let  $Y  =  \bigcup_{n<\omega} X_n$.  Clearly  $X_n \subseteq Y \subseteq A$, for  every 
$n< \omega$.  In  particular $X = X_0\subseteq Y$.  
Let us show that $Y$  is  a  subuniverse  of  $\alg A$.  
Let  $f$ be  a  basic  $k$-ary operation  and  $a \in Y^k$.  
From  the  construction  of  $Y$,  there  is  an  $n < \omega$
such  that $\forall i,\, a\,i \in X_n$.  From  its  definition,  
$f\, a \in X_{n+1} \subseteq Y$. 
Thus  $Y$ is  a  subuniverse  of  $\alg A$  containing  $X$.  
By~(\ref{eq:SgDef}), $\Sg^{\alg A}(X) \subseteq Y$.
For  the  opposite  inclusion,  it  is  enough  to  check,  by  induction  on $n$,  that 
$X_n \subseteq  \Sg^{\alg A}(X)$.  Well, $X_0  =  X  \subseteq  \Sg^{\alg A}(X)$  from  its  definition.  Assume  that $X_n \subseteq  \Sg^{\alg A}(X)$.  
If  $b \in X_{n+1}- X_n$,  then  $b  =  f \, a$ for a basic  $k$-ary 
operation $f$ and  $a \in X_n^k$.  But  $\forall i, \, a\, i  \in  \Sg^{\alg A}(X)$  and  since  this latter  object  is  a  subuniverse,  $b\in  \Sg^{\alg A}(X)$  as  well.
\end{proof}

\subsection{Subuniverses in Lean \href{https://github.com/UniversalAlgebra/lean-ualib/blob/master/src/subuniverse.lean}{(\lstinline{lean-ualib/subuniverse.lean})}}\

\noindent The  argument in the proof of Theorem~\ref{thm:1.14} is  of  a  type  that  one  
encounters  frequently  throughout  algebra.  It  has  two  parts.  First  that  $Y$  is  a 
subuniverse  containing  $X$.  Second  that  any  subuniverse  containing  $X$  must 
contain  $Y$.  Let us now see how this argument is formalized in Lean. 

\begin{lstlisting}
import basic
import data.set
namespace subuniverse
  section
    open set
    parameters {α : Type*} {S : signature} (A : algebra_on S α) 
    parameters {I : Type} {R : I → set α} 
    definition F := S.F
    definition ρ := S.ρ 

    -- Definition of subuniverse
    definition Sub (β : set α) : Prop :=
    ∀ (f : F) (a : ρ f → α), (∀ x, a x ∈ β) → A f a ∈ β
  
    -- Subuniverse generated by X
    definition Sg (X : set α) : set α := ⋂₀ {U | Sub U ∧ X ⊆ U}
\end{lstlisting}  
Here \lstinline{⋂₀ S} is notation for 
\begin{lstlisting}
sInter (S : set (set α)) : set α := λs, {a | ∀ t ∈ s, a ∈ t }}
\end{lstlisting}
So, if \lstinline{S : set (set α)}, a collection of sets of type \lstinline{α},
then \lstinline{⋂₀ S} is the intersection of the sets in \lstinline{S}.

Here are some helper definitions and theorems, the first two of which are already part of the standard Lean library.
\begin{lstlisting}
    -- Intersection introduction rule
    theorem Inter.intro {s : I → set α} : 
    ∀ x, (∀ i, x ∈ s i) → (x ∈ ⋂ i, s i) :=
    assume x h t ⟨a, (eq : t = s a)⟩, eq.symm ▸ h a
        
    -- Intersection elimination rule
    theorem Inter.elim {x : α} (C : I → set α) : 
    (x ∈ ⋂ i, C i) →  (∀ i, x ∈ C i) := 
    assume h : x ∈ ⋂ i, C i, by simp at h; apply h
        
    -- Intersection of subuniverses is a subuniverse
    lemma sub_of_sub_inter_sub (C : I → set α) : 
    (∀ i, Sub (C i)) → Sub ⋂i, C i :=
    assume h : ∀ i, Sub (C i), show  Sub (⋂i, C i), from 
      assume (f : F) (a : ρ f → α) (h₁ : ∀ x, a x ∈ ⋂i, C i), 
      show A f a ∈ ⋂i, C i, from 
        Inter.intro (A f a) 
        (λ j, (h j) f a (λ x, Inter.elim C (h₁ x) j))
        
    -- X is a subset of Sg(X)
    lemma subset_X_of_SgX (X : set α) : X ⊆ Sg X := 
    assume x (h : x ∈ X), 
    show x ∈ ⋂₀ {U | Sub U ∧ X ⊆ U}, from 
      assume W (h₁ : W ∈ {U | Sub U ∧ X ⊆ U}),  
      show x ∈ W, from 
        have h₂ : Sub W ∧ X ⊆ W, from h₁, 
        h₂.right h
        
    -- A subuniverse that contains X also contains Sg X
    lemma sInter_mem {X : set α} (x : α) : 
    x ∈ Sg X  →  ∀ {R : set α }, Sub R → X ⊆ R → x ∈ R := 
    assume (h₁ : x ∈ Sg X) (R : set α)  (h₂ : Sub R) (h₃ : X ⊆ R), 
    show x ∈ R, from h₁ R (and.intro h₂ h₃)
        
    -- Sg X is a Sub
    lemma SgX_is_Sub (X : set α) : Sub (Sg X) := 
    assume (f : F) (a : ρ f → α) (h₀ : ∀ i, a i ∈ Sg X), 
    show A f a ∈ Sg X, from 
      assume W (h : Sub W ∧ X ⊆ W), show A f a ∈ W, from 
        have h₁ : Sg X ⊆ W, from 
          assume r (h₂ : r ∈ Sg X), show r ∈ W, from 
            sInter_mem r h₂ h.left h.right,
            have h' : ∀ i, a i ∈ W, from assume i, h₁ (h₀ i),
            (h.left f a h')
        
        
    parameter (X : set α)
        
    inductive Y : set α
    | var (x : α) : x ∈ X → Y x
    | app (f : F) (a : ρ f → α) : (∀ i, Y (a i)) → Y (A f a)
        
    -- Y is a Sub
    lemma Y_is_Sub : (Sub Y) := 
    assume f a (h: ∀ i, Y (a i)), show Y (A f a), from 
      Y.app f a h 
        
    -- Y is the Sub generated by X
    theorem sg_inductive : Sg X = Y :=
    have h' : X ⊆ Y, from 
      assume x (h3 : x ∈ X), show x  ∈ Y, from Y.var x h3,
        
      have h : Sub Y, from 
        assume f a (h1 : ∀ x, Y (a x)), 
        show Y (A f a), from Y.app f a h1,
        
      have l : Sg X ⊆ Y, from 
        assume u (h3 : u ∈ Sg X), 
        show u ∈ Y, from (sInter_mem u) h3 h h',
        
      have r : Y ⊆ Sg X, from
        assume a (p: a ∈ Y), show a ∈ Sg X, from
        have q : a ∈ Y → a ∈ Sg X, from 
          Y.rec 
          --base: a = x ∈ X
          ( assume x (h1 : x ∈ X), 
            show x ∈ Sg X, from subset_X_of_SgX X h1 )
          --inductive: a = A f b for some b with ∀ i, b i ∈ Sg X
          ( assume f b (h2 : ∀ i, b i ∈ Y) (h3 : ∀ i, b i ∈ Sg X),
            show A f b ∈ Sg X, from SgX_is_Sub X f b h3 ),
        q p,
 
      subset.antisymm l r
 
 \end{lstlisting}  

Now we are ready to formalize the theorems and proofs from Section~\ref{sec:subuniverses}.

\begin{lstlisting}  
    parameter (X : set α)
  
    -- Sg X is a Sub
    theorem sub_SgX (X : set α) : Sub (Sg X) := 
    assume f a (h : ∀ i, a i ∈ Sg X), show A f a ∈ Sg X, from 
      assume U (h1 : Sub U ∧ X ⊆ U), show A f a ∈ U, from 
        have h3 : Sg X ⊆ U, from 
          assume r (h4 : r ∈ Sg X), show r ∈ U, from 
            sInter_mem r h4 h1.left h1.right,
        have h5 : ∀ i, a i ∈ U, from assume i, h3 (h i),
        (h1.left f a h5)
    

  
    inductive Y : set α
    | var (x : α) : x ∈ X → Y x
    | app (f : S.F) (a : S.ρ f → α) : (∀ i, Y (a i)) → Y (A f a)
  
    -- Y is a Sub
    lemma sub_Y : (Sub Y) := 
    assume f a (h: ∀ i, Y (a i)), show Y (A f a), from 
    Y.app f a h 
  
    -- Y is the smallest Sub containing X
    lemma sub_min_Y (U : set α) : Sub U → X ⊆ U → Y ⊆ U :=
    assume (h₁ : Sub U) (h₂ : X ⊆ U),
    assume (y : α)  (p : Y y), show U y, from 
      have q : Y y → Y y → U y, from 
        Y.rec
        --base: y = x ∈ X
        ( assume y (h : X y) (h' : Y y), h₂ h )
        --inductive: y = A f a for some a with ∀ i, a i ∈ Y
        ( assume f a (h : ∀ i, Y (a i)) (h' : ∀ i, Y (a i) → U (a i)) (h'' : Y (A f a)),
          have h₄ : ∀ i, a i ∈ U, from 
            assume i, h' i (h i), show U (A f a), from h₁ f a h₄ ),
      q p p
  
    -- Y is the Sub generated by X
    theorem sg_inductive : Sg X = Y :=
      have h' : X ⊆ Y, from 
        assume x (h3 : x ∈ X), show x  ∈ Y, from Y.var x h3,

      have h : Sub Y, from 
        assume f a (h1 : ∀ x, Y (a x)), show Y (A f a), from Y.app f a h1,

      have l : Sg X ⊆ Y, from 
        assume u (h3 : u ∈ Sg X), show u ∈ Y, from (sInter_mem u) h3 h h',
  
      have r : Y ⊆ Sg X, from
        assume a (p: a ∈ Y), show a ∈ Sg X, from
          have q : a ∈ Y → a ∈ Sg X, from 
            Y.rec 
            --base: a = x ∈ X
            ( assume x (h1 : x ∈ X), show x ∈ Sg X, from subset_X_of_SgX X h1 )
            --inductive: a = A f b for some b with ∀ i, b i ∈ Sg X
            ( assume f b (h2 : ∀ i, b i ∈ Y) (h3 : ∀ i, b i ∈ Sg X),
              show A f b ∈ Sg X, from sub_SgX X f b h3 ),
          q p,
      subset.antisymm l r
  end
  
end subuniverse
\end{lstlisting}  




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Terms       %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Terms and Free Algebras}
\label{sec:free-algebras}
% Let $\alg A = \<A, F^{\alg A}\>$ be a $S$-algebra. % with congruence lattice $\Con\<A, \dots \>$.
Recall that a \emph{clone} on a nonempty set $A$ is a set of operations on $A$
that contains the projection operations and is closed under general composition. 
Let $\sansC A$ denote the set of all clones on $A$.
The \emph{clone of term operations}
of a $S$-algebra $\alg A$, denoted by $\Clo \alg A$,
is the smallest clone on $A$ containing the basic operations of $\alg A$, that is,
\[
\Clo \alg A = \bigcap \{ U \in \sansC A \mid F^{\alg A} \subseteq U\}.
\]
The set of $n$-ary members of 
$\Clo \alg A$ is sometimes denoted by $\Clo_n \alg A$ % = \{t \in \Clo \alg A \mid \rho t = n\}$
(despite the fact that the latter is obviously not a clone).
The clone of \emph{polynomials} of $\alg A$, 
denoted by $\Pol \alg A$, is the clone generated by the basic operations
of $\alg A$ and the constant unary maps on $A$. The set of $n$-ary members of 
$\Pol \alg A$ is sometimes denoted by $\Pol_n \alg A$.
The smallest clone on a set $A$ is the set of all projections $\Proj A := \{\pi^n_i \mid 0\leq i < n < \omega\}$, defined as follows: for $0\leq i < n < \omega$, if  
$a \colon n \to A$, then $\pi^n_i a = a\, i$.
 
\begin{theorem}[\protect{\cite[Thm.~4.3.]{MR2839398}}] 
 Let $A$ be a set and $S = (F, \rho)$ a signature.
  Define
 \begin{align*}
  F_0 &= \Proj A;\\
 F_{n+1} &= F_n \cup \{ f g \mid f \in F, g \colon \rho f \to (F_n \cap (\rho g \to A)) \}, \text{ for } n < \omega.
 \end{align*}
 Then $\Clo^A(F) =  \bigcup_n F_n$.
\end{theorem}

\begin{lemma}
  \label{ex:1.16.6.brief} 
  Let $f$ and $g$ be homomorphisms from $\alg{A}$ to $\alg{B}$.
If $X \subseteq A$ and $X$ generates $\alg{A}$ and
    $\restr{f}{X} = \restr{g}{X}$, then $f = g$. 
\end{lemma}
\begin{proof}
  Suppose the subset $X \subseteq A$ generates $\alg{A}$ and suppose
  $\restr{f}{X} = \restr{g}{X}$.
  Fix an arbitrary element $a\in A$.  We show $f(a) = g(a)$.
  Since $X$ generates $\alg{A}$, there exists a (say, $n$-ary) term $t$ and 
  a tuple $(x_1, \dots, x_n) \in X^n$ such that 
  $a = t^{\alg{A}}(x_1, \dots, x_n)$. Therefore, 
  \begin{align*}
    f(a) = f(t^{\alg{A}}(x_1, \dots, x_n)) &= t^{\alg{B}}(f(x_1), \dots, f(x_n))\\
                                    &= t^{\alg{B}}(g(x_1), \dots, g(x_n))
                                     = g(t^{\alg{A}}(x_1, \dots, x_n)) = g(a).
  \end{align*}
  In other words, a homomorphism is uniquely determined by its restriction to 
  a generating set. 
\end{proof}

\begin{theorem}[\protect{\cite[Thm.~4.21]{MR2839398}}] 
  \label{thm:4.21}
  Let $\rho$ be a similarity type.
  \begin{enumerate}
    \item $\alg{T}_\rho(X)$ is generated by $X$.
    \item For every algebra $\alg{A}$ of type $\rho$ and every function 
    $h\colon X \to A$ there is a unique homomorphism $g\colon \alg{T}_\rho(X) \to \alg{A}$ 
    such that $\restr{g}{X} = h$.
  \end{enumerate}
\end{theorem}
\begin{proof} The definition of $\alg{T}_\rho(X)$ exactly parallels the construction in 
Theorem 1.14. That accounts for (1). For (2), define $g(t)$ by induction on $|t|$. 
Suppose $|t| = 0$. Then $t \in X \cup \class{F}_0$. If $t \in X$ then define 
$g(t) = h(t)$. For $t \in \class{F}_0$, $g(t) = t^{\alg{A}}$. 
Note that since $\alg{A}$ is an algebra of type $\rho$ and $t$ is a nullary 
operation symbol, $t^{\alg{A}}$ is defined.

For the inductive step, let $|t| = n + 1$. Then $t = f(s_1, \dots, s_k)$ for some 
$f \in \class{F}_k$ and $s_1, \dots, s_k$ each of height at most $n$. We define
$g(t) = f^{\alg{A}}(g(s_1), \dots, g(s_k))$.

By its very definition, $g$ is a homomorphism.
Finally, the uniqueness of $g$ follows from Lemma~\ref{ex:1.16.6.brief}. 
\end{proof}


\subsection{Terms and Free Algebras in Lean \href{https://github.com/UniversalAlgebra/lean-ualib/blob/master/src/free.lean}{(\lstinline{lean-ualib/free.lean})}}\

\noindent We now demonstrate Lean's support of inductive types, which are 
essential for defining infinite objects in a constructive and computable way. 
We define an inductive type that constructs the (infinite) 
collection $\mathbb T (X)$ of all terms of a given signature.
\begin{lstlisting}
  import basic
  section
    parameters {S : signature} (X :Type*) 
    local notation `F` := S.F
    local notation `ρ` := S.ρ 
  
    inductive term
    | var : X → term
    | app (f : F) : (ρ f → term) → term

    def Term : algebra S := ⟨term, term.app⟩
  end
\end{lstlisting}
The set of terms along with the operations $F^{\alg T} := \{\mathtt{app \,f} \mid f : F\}$ forms an algebra $\alg T(X) = \< \mathbb T(X), F^{\alg T}\>$ in the signature $S = (F, \rho)$. Suppose $\alg A = \< A, F^{\alg A}\>$ is an algebra in the same signature and $h \colon X \to A$ is an arbitrary function.  We will show that $h \colon X \to A$ has a unique \emph{extension} (or \emph{lift}) to a homomorphism from $\mathbf T(X)$ to $\alg A$.  Since $\alg A$ and $h \colon X \to A$ were aribtrary, this unique homomorphic lifting property holds universally; accordingly we say that the term algebra $\alg T(X)$ is \emph{absolutely free} for the signature $S$.
Before implementing the formal proof of this fact in Lean, let us first define some convenient ``syntactic sugar.''
\begin{lstlisting}
section
  open term
  parameters {S : signature} (X :Type*) {A : algebra S}
  definition F := S.F         -- operation symbols
  definition ρ := S.ρ         -- arity function
  definition 𝕋 := @Term S     -- term algebra over X
  definition 𝕏 := @var S X    -- generators of the term algebra
\end{lstlisting}
If \lstinline{h : X → A} is a function defined on the generators
of the term algebra, then the \emph{lift} (or \emph{extension}) of  \lstinline{h} 
to all of \lstinline{𝕋(X)} is defined inductively as follows:
\begin{lstlisting}
  definition lift_of (h : X → A) : 𝕋(X) → 
  | (var x) := h x
  | (app f a) := (A f) (λ x, lift_of (a x))
\end{lstlisting}
  To prove that the term algebra is absolutely free, we show that
  the lift of an arbitrary function \lstinline{h : X → A} is a homomorphism
  and that this lift is unique.
  \begin{lstlisting}
  -- The lift is a homomorphism.
  lemma lift_is_hom (h : X → A) : homomorphic (lift_of h) :=
  λ f a, show lift_of h (app f a) = A f (lift_of h ∘ a), from rfl

  -- The lift is unique.
  lemma lift_is_unique : ∀ {h h' : 𝕋(X) → A},
  homomorphic h → homomorphic h' → h ∘ 𝕏 = h' ∘ 𝕏 → h = h' :=
  assume (h h' : 𝕋(X) → A) (h₁ : homomorphic h)
    (h₂ : homomorphic h')(h₃ : h ∘ 𝕏 = h' ∘ 𝕏),
    show h = h', from 
      have h₀ : ∀ t : 𝕋(X), h t = h' t, from 
        assume t : 𝕋(X), 
        begin
          induction t with t f a ih₁ ,
          show h (𝕏 t) = h' (𝕏 t),
          { apply congr_fun h₃ t },

          show h (app f a) = h' (app f a),
          { have ih₂  : h ∘ a = h' ∘ a, from funext ih₁,
            calc h (app f a) = A f (h ∘ a) : h₁ f a
                         ... = A f (h' ∘ a) : congr_arg (A f) ih₂ 
                         ... = h' (app f a) : (h₂ f a).symm }
        end,
      funext h₀ 
end
\end{lstlisting}









%|||||||||||||||||||||||||||| OMIT BELOW |||||||||||||||||||||||||||||||||||||||
%VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV

\ifextver

\begin{theorem}[\protect{\cite[Thm.~4.32]{MR2839398}}] 
  \label{thm:4.32} 
Let $\alg{A}$ and $\alg{B}$ be algebras of type $\rho$.
\begin{enumerate}
  \item
    For every $n$-ary term $t$ and homomorphism $g\colon \alg{A} \to \alg{B}$, 
    we have 
    $g(t^{\alg{A}}(a_1,\dots, a_n)) = t^{\alg{B}}(g(a_1),\dots, g(a_n))$.
  \item
    For every term $t \in T_\rho(X_\omega)$ and every $\theta \in \Con(\alg{A})$, we have
    $\va \equiv_\theta \vb \implies t^{\alg{A}}(\va) \equiv_\theta t^{\alg{A}}(\vb)$.
  \item
    For every subset $Y$ of $A$, we have
    \[\Sg^{\alg{A}}(Y) = 
    \{ t^{\alg{A}}(a_1,\dots, a_n) : t \in T(X_n), a_i \in Y, i \leq n < \omega\}.\]
\end{enumerate}
\end{theorem}
\begin{proof} 
  The first statement is an easy induction on $|t|$. 
  The second statement follows from the first by taking $\alg{B} = \alg{A}/\theta$ 
  and $g$ the canonical homomorphism. For the third statement, again by induction on the 
  height of $t$, every subalgebra must be closed under the action of $t^{\alg{A}}$. 
  Thus the right-hand side is contained in the left. On the other hand, the right-hand 
  side is clearly a subalgebra containing the elements of $Y$ (take $t = x_1$) from 
  which the reverse inclusion follows.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{A Case Study: formalizing Birkhoff's Theorem}

\subsection{Elementary facts}
\begin{lemma}[\protect{\cite[Ex.~1.16.6]{MR2839398}}]
  \label{ex:1.16.6} 
  Let $f$ and $g$ be homomorphisms from $\alg{A}$ to $\alg{B}$.
  Let $E(f,g) = \{ a \in A : f(a) = g(a) \}$ (the \emph{equalizer} of $f$ and $g$). 
  \begin{enumerate}
    \item	$E(f,g)$ is a subuniverse of $\alg{A}$.
    \item If $X \subseteq A$ and $X$ generates $\alg{A}$ and
    $\restr{f}{X} = \restr{g}{X}$, then $f = g$. 
    \item If $\alg{A}, \alg{B}$ are finite and $X$ generates $\alg{A}$, 
    then $|\!\Hom{\alg{A},\alg{B}}| \leq |B|^{|X|}$.
  \end{enumerate}
\end{lemma}
\begin{proof}
  Let $\rho$ be the similarity type of $\alg{A}$ and $\alg{B}$, and 
  $p$ a (say, $n$-ary) operation symbol in $\rho$. Then, 
  for every tuple $(a_1, \dots, a_n) \in E(f,g)^n$,
  \begin{align*}
    f(p^{\alg{A}}(a_1, \dots, a_n)) &= p^{\alg{B}}(f(a_1), \dots, f(a_n))\\
                                    &= p^{\alg{B}}(g(a_1), \dots, g(a_n))
                                     = g(p^{\alg{A}}(a_1, \dots, a_n)).
  \end{align*}
  Therefore, $E(f,g)$ is closed under $p$.  Since $p$ was arbitrary, 
  $E(f,g)$ is closed under all operations in $\rho$ and is thus a 
  subuniverse of $\alg{A}$.

  Suppose the subset $X \subseteq A$ generates $\alg{A}$ and suppose
  $\restr{f}{X} = \restr{g}{X}$.
  Fix an arbitrary element $a\in A$.  We show $f(a) = g(a)$.
  Since $X$ generates $\alg{A}$, there exists a (say, $n$-ary) term $t$ and 
  a tuple $(x_1, \dots, x_n) \in X^n$ such that 
  $a = t^{\alg{A}}(x_1, \dots, x_n)$. Therefore, 
  \begin{align*}
    f(a) = f(t^{\alg{A}}(x_1, \dots, x_n)) &= t^{\alg{B}}(f(x_1), \dots, f(x_n))\\
                                    &= t^{\alg{B}}(g(x_1), \dots, g(x_n))
                                     = g(t^{\alg{A}}(x_1, \dots, x_n)) = g(a).
  \end{align*}
  In other words, a homomorphism is uniquely determined by its restriction to 
  a generating set. There are exactly $|B|^{|X|}$ functions from $X$ to $B$ so, 
  assuming $X$ generates $\alg{A}$, we have
  $|\!\Hom{\alg{A},\alg{B}}| \leq |B|^{|X|}$.
\end{proof}

\begin{lemma}[\protect{\cite[Ex.~1.26.8]{MR2839398}}]
  \label{ex:1.26.8}
  Suppose $f \in \Hom{\alg{A},\alg{B}}$, $g \in \Hom{\alg{A},\alg{C}}$, 
  $f$ is surjective, and $\ker f \subseteq \ker g$, then $\exists h \in \Hom{\alg{B},\alg{C}}$, $g = h \compose f$.
  % Let $f \colon \alg{A} \to \alg{B}$ and $f \colon \alg{A} \to \alg{C}$ be homomorphisms, with $g$ surjective. Prove that if $\ker g \subseteq \ker f$, then there is a homomorphism 
  % $h \colon \alg{C} \to \alg{B}$ such that $f = h \compose g$.
\end{lemma}
\begin{proof}
Define $h\colon B \to C$ as follows: for each $b\in B$, choose (by Axiom of Choice!) $a_0\in f^{-1}\{b\}$
and let $h(b) = g(a_0)$.  (Since $f$ is surjective, such an $a_0$ exists for each $b\in B$.)
Fix $a \in A$.  We show $g(a) = h f(a)$. Let $a_0$ be the element of $f^{-1}\{f(a)\}$ that we
chose when defining $h$ at $b = f(a)$. That is, $h(b) = g(a_0)$.
Then, $f(a_0) = b = f(a)$, so $(a_0, a) \in \ker f\subseteq \ker g$, so 
$g(a) = g(a_0) = h(b) = h f(a)$, as desired.

To see that $h$ is a homomorphism, let $p$ be a (say, $n$-ary) operation symbol.
Let $(b_1, \dots, b_n) \in B^n$, and let $(a_1, \dots, a_n)$ be the respective representatives
of the $f$-kernel classes $f^{-1}\{b_i\}$ that we chose when defining $h$.
Then,
\begin{align*}
p^{\alg{C}}(h(b_1), \dots, h(b_n)) &=  p^{\alg{C}}(h f(a_1), \dots, h f(a_n))\\
&=  p^{\alg{C}}(g(a_1)), \dots, g(a_n))\\
&=  g p^{\alg{A}}(a_1, \dots, a_n))\\
&=  h f p^{\alg{A}}(a_1, \dots, a_n)\\
&=  h p^{\alg{B}}(f(a_1), \dots, f(a_n))\\
&=  h p^{\alg{B}}(b_1, \dots, b_n).
\end{align*}
\end{proof}


\begin{definition}
Let $\rho$ be a similarity type. 
An \emph{identity of type} $\rho$ is an ordered pair of terms, written 
$p \approx q$, from $T_\rho(X_\omega)$. Let $\alg{A}$ be an algebra of type $\rho$.
We say that $\alg{A}$ satisfies $p\approx q$ if $p^{\alg{A}} = q^{\alg{A}}$. 
In this situation, we write $\alg{A} \models p \approx q$.
If $\class{K}$ is a class of algebras of type $\rho$, we write 
$\class{K} \models p \approx q$ if $\forall \alg{A} \in \class{K}$, 
$\alg{A} \models p \approx q$. Finally, if $\Sigma$ is a set of equations, 
we write $\class{K} \models \Sigma$ if every member of $\class{K}$ satisfies
every member of $\Sigma$.
\end{definition}

\begin{definition} Let $\class{K}$ be a class of algebras and $\Sigma$ a set of equations, each 
of similarity type $\rho$. We define 
$\Id{\class{K}} = \{p \approx q : \class{K} \models p \approx q\}$ 
and
$\Mod{\Sigma} = \{ \alg{A} : \alg{A} \models \Sigma \}$.
Classes of the form $\Mod{\Sigma}$ are called \emph{equational classes}, and $\Sigma$ is called 
an \emph{equational base} or an \emph{axiomatization} of the class. $\Mod{\Sigma}$ is called the 
class of \emph{models} of $\Sigma$. Dually, a set of identities of the form $\Id{\class{K}}$ is called an 
\emph{equational theory}.
\end{definition}

\begin{lemma}[\protect{\cite[Lem.~4.36]{MR2839398}}] 
  \label{lem:4.36} 
  For every class $\class{K}$, each of the classes $\sansS \class{K}$, 
  $\sansH \class{K}$, $\sansP \class{K}$, and $\clop{V} \class{K}$ satisfies 
  exactly the same identities as does $\class{K}$.
\end{lemma}
\begin{proof} (exercise) \end{proof}


\begin{lemma}[\protect{\cite[Lem.~4.37]{MR2839398}}]
  \label{lem:4.37} 
  $\class{K} \models p \approx q$ 
  if  and  only  if  for  every  $\alg{A} \in \class{K}$  and  every  
  $h\in \Hom{\alg{T}(X_\omega),\alg{A}}$,  we  have  $h(p)  =  h(q)$.
\end{lemma} 
\begin{proof} First  assume  that  $\class{K} \models p\approx  q$.  
  Pick  $\alg{A}$ and $h$ as  in  the  theorem.  Then
  $\alg{A} \models p\approx q \implies p^{\alg{A}} = q^{\alg{A}} \implies 
  p^{\alg{A}}(h(x_1), \dots, h(x_n)) = q^{\alg{A}}(h(x_1), \dots, h(x_n))$.
  Since  $h$  is  a  homomorphism,  we  get  
  $h(p^{\alg{A}}(x_1, \dots, x_n)) = h(q^{\alg{A}}(x_1, \dots, x_n))$, i.e., $h(p) = h(q)$.

  To  prove  the  converse  we  must  take  any  $\alg{A} \in \class{K}$  and  
  $a_1, \dots, a_n \in A$ and show that $p^{\alg{A}}(x_1, \dots, x_n) = q^{\alg{A}}(x_1, \dots, x_n)$.
  Let $h_0 \colon X_\omega \to A$ be a function with $h_0(x_i) = a_i$ for $i\leq n$.
  By  Theorem~\ref{thm:4.21},  $h_0$ extends  to  a homomorphism  $h$ from $\alg{T}(X_\omega)$
  to $\alg{A}$.  By  assumption  $h(p)  =  h(q)$.  Since 
  $h(p)  =  h(p^{\alg{A}}(x_1, \dots, x_n)) = 
  p^{\alg{A}}(h(x_1), \dots, h(x_n)) =  p^{\alg{A}}(a_1,\dots, a_n)$ 
  (and  similarly  for  $q$)  the  result  follows. 
\end{proof}

{\bf Notation.}
\emph{In what follows, $\alg F$ will always denote $\alg{F}_{\class{K}}$, unless state otherwise.}

\begin{theorem}[\protect{\cite[Thm.~4.38]{MR2839398}}]
  \label{thm:4.38}  
  Let $\class{K}$ be a class of algebras and $p \approx q$ an equation. The 
  following are equivalent.
  \begin{enumerate}
    \item \label{item:1} $\class{K} \models p\approx q$.
    \item \label{item:2} $(p,q)$ belongs to the congruence $\lambda_{\class{K}}$ on $\alg{T}(X_\omega)$.
    \item \label{item:3} $\alg F(X_\omega) \models p\approx q$.
  \end{enumerate} 
\end{theorem}
\begin{proof} 
  We shall show 
  (\ref{item:1}) $\implies$ (\ref{item:3}) $\implies$ (\ref{item:2}) $\implies$ (\ref{item:1}).
  Throughout the proof we write $\alg{F}$ for $\alg F(X_\omega)$, $\alg{T}$ for 
  $\alg{T}(X_\omega)$ and $\lambda$ for $\lambda_{\class{K}}$. 
  Recall that $\alg{F} = \alg{T}/\lambda \in \sansS\sansP(\class{K})$. 
  From (a) and Lemma~\ref{lem:4.36} we get $\sansS\sansP(\class{K}) \models p \approx q$. 
  Thus (c) holds.

  From (c), $p^{\alg{F}}(\bar{x}_1,\dots, \bar{x}_n) = q^{\alg{F}}(\bar{x}_1,\dots, \bar{x}_n)$
  where $\bar{x}_i = x_i/\lambda$. From the definition of $\alg{F}$, 
  $p^{\alg{T}}(x_1,\dots, x_n) \equiv_\lambda q^{\alg{T}}(x_1,\dots, x_n)$
  from which (b) follows since $p = p^{\alg{T}}(x_1,\dots, x_n)$ and $q = q^{\alg{T}}(x_1,\dots, x_n)$.

  Finally assume (b). We wish to apply Lemma~\ref{lem:4.37}. 
  Let $\alg{A} \in \class{K}$ and $h \in \Hom{\alg{T},\alg{A}}$. 
  Then $\alg{T}/\ker h \in \sansS(\alg{A}) \subseteq \sansS(\class{K})$ so $\ker h \supseteq \lambda$. 
  Then (b) implies that $h(p) = h(q)$ hence (a) holds.
\end{proof}

Theorem~\ref{thm:4.38} tells us that we can determine whether an identity is true 
in a variety by consulting a particular algebra, namely $\alg{F}(X_\omega)$. 
Sometimes it is convenient to work with algebras free on other generating sets besides 
$X_\omega$. The following corollary takes care of that for us.

\begin{corollary}[\protect{\cite[Cor.~4.39]{MR2839398}}]
  \label{cor:4.39} 
Let $\class{K}$ be a class of algebras, $p$ and $q$ $n$-ary terms, $Y$ a set 
and $y_1, \dots, y_n$ distinct elements of $Y$. Then $\class{K} \models p \approx q$ 
if and only if 
$p^{\alg F(Y)}(y_1, \dots, y_n) = q^{\alg F(Y)}(y_1, \dots, y_n)$.
In particular, $\class{K} \models p \approx q$ if and only if 
$\alg F(X_n)\models p \approx q$.
\end{corollary}
\begin{proof}
Since $\alg F(Y)\in \sansS\sansP(\class{K})$, the left-to-right 
direction uses the same argument as in Theorem~\ref{thm:4.38}. So assume 
that 
$p^{\alg F(Y)}(y_1, \dots, y_n) = q^{\alg F(Y)}(y_1, \dots, y_n)$.
To show that $\class{K} \models p \approx q$, let 
$\alg{A} \in \class{K}$ and $a_1$, $\dots$, $a_n \in A$. 
We must show $p^{\alg{A}}(a_1, \dots, a_n) = q^{\alg{A}}(a_1, \dots, a_n)$. 
There is a homomorphism $h\colon \alg F(Y) \to \alg{A}$ 
such that $h(y_i) = a_i$ for $i \leq n$. Then
\begin{align*}
 p^{\alg{A}}(a_1, \dots, a_n) &= p^{\alg{A}}(h (y_1), \dots, h (y_n))
= h(p^{\alg F(Y)}(y_1, \dots, y_n))\\
&= h(q^{\alg F(Y)}(y_1, \dots, y_n))
= q^{\alg{A}}(h(y_1), \dots, h(y_n))\\
&= q^{\alg{A}}(a_1, \dots, a_n).
\end{align*}
ese general goals 
\end{proof}

It follows from Lemma~\ref{lem:4.36} that every equational class is a variety. 
The converse is Birkhoff's Theorem.

\begin{theorem}[\protect{\cite[Thm.~4.41]{MR2839398}}]
  \label{thm:4.41}
Every variety is an equational class.
\end{theorem}
\begin{proof}
Let $\var{W}$ be a variety. We must find a set of equations that axiomatizes $\var{W}$. 
The obvious choice is to use the set of all equations that hold in $\var{W}$. 
To this end, take $\Sigma = \Id{\var{W}}$. Let $\close{\var{W}} = \Mod{\Sigma}$. 
Clearly, $\var{W} \subseteq \close{\var{W}}$. We shall prove the reverse inclusion.

Let $\alg{A} \in \close{\var{W}}$ and $Y$ a set of cardinality $\max(|A|, |\omega|)$. 
Choose a surjection $h_0\colon Y \to A$. By Theorem~\ref{thm:4.21}, $h_0$ extends to 
a (surjective) homomorphism $h \colon \alg{T}(Y) \to \alg{A}$. Furthermore, since 
$\alg{F}_{\var{W}}(Y) = \alg{T}(Y)/\Theta_{\var{W}}$, there is a surjective homomorphism
$g \colon \alg{T}(Y) \to \alg{F}_{\var{W}}$.

We claim that $\ker g \subseteq \ker h$. 
If the claim is true then by Lemma~\ref{ex:1.26.8} there is a map 
$f\colon \alg{F}_{\var{W}}(Y) \to \alg{A}$ such that $f \compose g = h$. 
Since $h$ is surjective, so is $f$. Hence 
$\alg{A} \in \sansH(\alg{F}_{\var{W}}(Y)) \subseteq \var{W}$ completing the proof.

Let $u,v \in T(Y)$ and assume that $g(u) = g(v)$. 
Since $\alg{T}(Y)$ is generated by $Y$, by Theorem~\ref{thm:4.21}, 
there is an integer $n$, terms $p, q \in T(X_n)$, and 
$y_1$, $\dots$, $y_n \in Y$ such that $u = p^{\alg{T}(Y)}(y_1,\dots, y_n)$ and 
$v = q^{\alg{T}(Y)}(y_1,\dots, y_n)$, by Theorem~\ref{thm:4.32}. 
Applying the homomorphism $g$,
\[
 p^{\alg{F}_{\var{W}}(Y)}(y_1,\dots, y_n) = g(u) = g(v) = 
 q^{\alg{F}_{\var{W}}(Y)}(y_1,\dots, y_n).
 \]
 Then by Corollary~\ref{cor:4.39}, $\var{W} \models p \approx q$, hence 
 $(p \approx q) \in \Sigma$. 
 Since $\alg{A} \in \close{\var{W}} = \Mod{\Sigma}$,
 we get $\alg{A} \models p \approx q$. Therefore, 
 \[
  h(u) = p^{\alg{A}}(h_0(y_1), \dots, h_0(y_n))
  = q^{\alg{A}}(h_0(y_1), \dots, h_0(y_n)) = h(v),
  \]
 as desired.
\end{proof}

\subsection{Book project}  This experience was part of my motivation to 
initiate a book project called ``Algebras, Categories and Types: with computer-aided proofs,'' 
which aims to not only explain the subjects of the title (in a constructive way when possible),
but also to exhibit Lean code that formalizes the important definitions and theorems.
We anticipate that readers will end up with both a deeper understanding of
the theory and enough knowledge about Lean to support future exploration of these
mathematical subjects.


\fi
% /\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\
% | | | | | | | | | | | | | | |  OMIT ABOVE | | | | | | | | | | | | | | | | | | | | | | 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Lean in the Classroom}

I was fortunate enough to be able to teach a graduate course in Model Theory
in the Spring 2018 semester University of Colorado, Boulder.  This year I am teaching Abstract Algebra in the Fall of 2018, and Discrete Math (which serves as an ``intro to proofs'') in both the Fall and Spring semesters of the 2018/19 academic year.  My experience with these courses has convinced
me that Lean has an important place in logic classroom, even at the undergraduate level.

In the Fall 2018, I presented logic to my first- and second-year undergraduate students using Gentzen style natural deduction almost exclusively. (Of course, the students were also exposed to classical logic and truth tables, and I taught them the not-so-subtle differences between classical and constructive reasoning principles.)
Because natural deduction involves only a very small set of concrete derivation rules, and essentially no axioms, it is ideal for getting the students up and proving with first-order logic. The students seem more confident and satisfied with their proofs than the students in previous courses, when the treatment was more traditional, based entirely on set theory, truth tables, and proof sequences.

Another benefit of using Gentzen style natural deduction
was that it enabled the students to better appreciate \emph{soundness} and \emph{completeness}.  
It seems easier to explain these concepts to students who have some
experience building proof derivation trees (that are purely syntactic) on the one hand, while on the other hand having some experience with truth values and what it means for a formula to be true in a particular model.

Evidently, Paul Taylor has witnessed a similar phenomenon, as
he descirbes in~\cite{MR1694820}, where he refers to ``proof boxes'' (essentially natural deduction diagrams).
\begin{quote}
``I have seen logic introduced to first year undergraduates
both in the form of truth-assignments and using proof boxes, and firmly
believe that the box method is preferable... % As this section has shown,
it is a formal version of the way mathematicians
actually reason, even if they claim to use Boolean algebra when asked.''
\end{quote}
% \phantom{x} \hfill --Paul Taylor~\cite[Remark 1-6-14]{MR1694820}.
About half of the students in my undergraduate class were computer science majors, and most of them have had some prior programming experience.
So, I decided to introduce Lean in the classroom in order to keep the students  more engaged and to further develop their logical agility and reinforce their understanding of the reciprocal roles played by the introduction and elimination rules of natural deduction.
At the same time, I did not want to alienate any students who had no experience 
with, and possibly some aversion to, computer programming.
As a compromise I introduced Lean in the classroom via team homework
assignments. As it turns out, their Lean proofs were among their best proofs!  They became quite skilled at constructing formal proofs in Lean, and their confidence and understanding of the analogous paper-and-pencil proofs seemed much stronger after having worked with the proof assistant.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary of project stages}
In the introduction, we set out the goals of the project in broad strokes. 
Here we describe four stages of concrete activities that map out a plan for 
achieving our goals.



We briefly summarize the high-level goals of the project
in the following list of four key stages. The presentation here is brief, and the language
over-simplified, but further details about each of these stages are presented in later sections.

\begin{enumerate}
\item[Stage 1.] {\bf lean-ualib.} Implement in Lean (the proof assistant described below) \textsl{formal
 statements and proofs} of the definitions and theorems that constitute
  the \textsl{core} of our field (universal algebra and lattice theory).
  (We call this the \textsl{Lean Universal Algebra Library}, or {\tt lean-ualib} for short.)\footnote{Work on the {\tt lean-ualib} has already begun; the permanent residence of 
  our open source repository is \url{https://github.com/UniversalAlgebra/lean-ualib}.}
\item[Stage 2.] {\bf domain-specific automation.} Develop \textsl{proof tactics} in Lean that carry out, in a highly automated way, the most common arguments and proof techniques of universal algebra; that is, make the \textsl{proof idioms} of our field available in Lean.

\item[Stage 3.] {\bf Algebras, Categories and Types: with computer-aided proofs.} Publish a \textsl{textbook} that presents (a) the theory that is formalized
  in {\tt lean-ualib}, (b) 
  a comprehensive \textsl{reference manual} for the library, and (c) \emph{examples and
  instructions for working mathematicians}.
\item[Stage 4.] {\bf lean-uaailib.}  \textsl{Develop search and artificial intelligence tools that
  will accelerate growth of the library either by guided user input or by allowing the
  library to grow itself.}
\end{enumerate}  
 
Although Stage 1 alone may seem like a massive undertaking, we will start with the
basic results of the theory and formalize the lemmas and theorems that are most commonly
used in our field to prove deeper results.
In the process, we will
\begin{enumerate}
  \item figure out how to \emph{automate proof search} in
    the specific domain of universal algebra;
    \item create libraries for \emph{clones} of operations and terms,
free algebras, homomorphisms and congruences, and design reasoning procedures for these;
\item explore how to best exploit Lean's \emph{metaprogramming framework} and develop
techniques and tools that help mathematicians perform accurate deductions and computations
using the proof assistant;
\item integrate procedures from other computer algebra systems (in particular, the UACalc)
  in a formally verified way.
  \item contribute to Lean's development with ideas and features designed
    to benefit all users, notably an efficient built-in procedure for
    first-order reasoning and an integration with automatic theorem provers.
\end{enumerate}

Then we will go after the deeper results, prioritizing definitions, lemmas, and theorems
according to how widely they are used in our field.
With these foundations established, we will soon have a small mathematical arsenal 
at our disposal that we can exploit when formalizing proofs of deeper theorems.
Thus, the library will expand until we have formalized the core of our subject.

With this strategy, we expect the library development will begin at a moderate pace
but will quickly accelerate.  To ensure that we don't get off to a slow start,
we will develop novel automation strategies, and ``proof tactics'' that are specifically
tailored to the type of arguments that are most commonly used---the ``proof idioms'' of our field
Furthermore, we will build upon already existing Lean math libraries, such as the Lean 
Mathlib~\cite{lean-mathlib:2018}, so we will not have to formalize all of the basic 
theorems from scratch. These strategies will combine to substantially reduce and limit the amount of work required
to complete the first stage.
%% If I don't spend the majority of my research time and energy
%% on nontrivial aspects of a problem, if I'm often occupied checking, correcting, and re-checking
%% my own work, or refereeing others' work, then I am a fool. (Worse than a fool, I'm 
%% an \emph{over-educated} fool.)

The ultimate goal is to advance the state of Lean and its mathematical libraries
to the point at which it becomes
\emph{a proof assistant that helps working mathematicians}, by making them both
more productive and more confident in their results.



%% \section{Background: proof assistants}
%% Proof assistants (also called interactive theorem provers) are increasingly used in academia and
%% industry to verify the correctness of hardware, software, and protocols. However, despite the
%% trustworthiness guarantees they offer, most mathematicians find them too laborious to use.

%% The goal of this project is to collaborate with number theorists to formally prove theorems about
%% research mathematics and to address the main usability issues hampering the adoption of proof
%% assistants in mathematical circles. The theorems will be selected together with our collaborators
%% to guide the development of formal libraries and verified tools.

%% \bigskip

%% {\bf TODO:} the above is a direct quote from Jasmin; tailor it to our proposal.

%% \bigskip


%% Proof assistants are powerful, trustworthy tools. If we computer scientists care to
%% listen to interested mathematicians and cooperate with them, we can learn how
%% to direct our tools towards their goals.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Challenges and objectives}
Interactive theorem proving is steadily gaining ground. In some areas
of computer science, it is common for research papers to be accompanied by formal proofs. Proof assistants are even deployed in the classroom [51, 57], replacing paper-and-pencil arguments. These circumstances point to a future where these tools will be routinely used, resulting in more reliable science.
But to have a substantial impact on mathematical practice, we must narrow the usability gap.

\begin{quote}
Our overall aim is to make proof assistants usable by mathematicians, by initially focusing on number theory and related areas, developing the proof automation, tool integrations, and formal libraries guided by actual users' needs.
\end{quote}

The difficulties are as much social as technical. Proof assistants are developed primarily by computer scientists. Much of the formalized mathematics is motivated by hardware or software verification [17, 37, 40, 42, 61]. Mathematicians largely dismiss proof assistants as impractical, so the technology improves only slowly. Sadly, even routine operations such as factorization of polynomials can become small challenges when moving to a formal-logic environment [46, 66]. We want to break the vicious circle by working closely with mathematicians. We aim to bring a proof assistant, its automation, and its libraries further, guided by the needs of mathematicians who understand the value of proof assistants. Specifically, we will collaborate with Sander Dahmen and his team at the VU Amsterdam to formalize parts of the team's research in number theory and recent results in related areas, addressing usability issues as they arise.

Our vehicle will be the Lean proof assistant [27]. Lean is a new open source system developed by Leonardo de Moura (Microsoft Research, \usa), Jeremy Avigad (Carnegie Mellon, \usa), and their colleagues. The system's design and engineering is unusually clean and efficient. Lean attempts to combine the best from two leading proof assistants.

\begin{itemize}
\item Lean's logical foundation is a variant of Coq's calculus of inductive constructions (\cic)~\cite{MR935892}, a dependent type theory. Lean distinguishes itself with its small inference kernel and strong automation. Independent proof checkers provide additional guarantees. Lean's support for dependent types is smoother than Coq's, thanks to flexible pattern matching and a generalized congruence closure algorithm~\cite{MR3536762}. A mechanism for introducing quotient types and a transfer tool facilitate reasoning up to isomorphism without resorting to setoids [6] or homotopy type theory [5].
\item For the design of basic algebraic libraries, Lean's developers turned to Isabelle/HOL for inspiration. The libraries rely on type classes, a mechanism to categorize types and their operations (e.g., ``$\langle \mathbb Z, 0, -, +\rangle$ forms a group''). Type classes interact well with Lean's dependent types. By contrast, in Isabelle/HOL, there is no way to use type classes to reason about the integers modulo $n$ as a ring, as observed by Avigad et al. [3, Section 3.1]. I encountered similar difficulties with
tensors and multidimensional measures [8, Section 5].
\end{itemize}

Our overall aim will be met by pursuing four scientific objectives, presented below. Our starting point is that there is tremendous value in simultaneously using and developing a proof assistant. Part of the project's innovative character is that it combines these two activities, which are normally carried out by different people working in different teams. Especially for a young s
ystem, close cooperation between developers and users is essential. Although formalization and implementation work can be satisfying in their own right, we will do everything possible to avoid sterile exercises in formalization and development of a large but useless library. Our first objective is to formally prove theorems about research mathematics.

From the mathematician's point of view, formalization can increase the trustworthiness of their results and the lucidity of their proofs, while raising awareness of formal verification in the mathematics community. From a computer science perspective, formalization will guide the development of proof assistant technology.

We will formalize theorems emerging from our research in universal algebra and lattice theory. The PI is involved in three different research projects, two in universal algebra and one that involves both algebra and lattice theory.  These projects are briefly described in the appendix below. A proof assistant equipped with special libraries and tactics for formalizing universal algebra could be extremely useful for these research problems. One of our goals is to demonstrate
the utility of such libraries and tactic for proving these ``real-world'' theorems.

Furthermore, a number of results and ideas on which our work depends appear in journals and conference proceedings with many important details missing.  Using Lean allows us to not only verify the correctness of theorems and proofs, but
also determine the precise foundational assumptions required to confirm the validity of the result. Thus, when doing mathematics with the help of modern proof technology,  we are presented with the possibility of automated process for generalizing results.

The idea is that implementing theorems as types
and proofs as ``proof objects'' would produce the following:
\begin{enumerate}
\item computer verified, and possibly simplified, proofs of known results
\item better understanding of existing theory and algorithms
\item new and/or improved theorems and algorithms
\end{enumerate}

There is substantial evidence that Lean is the best platform for formalizing universal algebra.  The type theory on which Lean is based provides a foundation for computation that is more powerful than first-order logic and seems well suited to the specification of the basic objects and most common proof strategies found in of our field.

In first-order logic, higher-level (meta) arguments are second class citizens:
they are interpreted as informal procedures that should be expanded to primitive
arguments to achieve full rigor. This is fine for informal proofs, but becomes
impractical in a formal one. Type theory supplies a much more satisfactory
solution, by providing a language that can embed meta-arguments in types. 

Indeed, algebraic structures are most naturally specified using typed predicate calculus expressions and explicitly requires variable-dependent sets, just like the specification language of Martin-L\"{o}f's Theory of Types~\cite{MR769301}.
In order to support this feature, programming constructs should be able to return set or type values, hence \emph{dependent types}. This accommodates a stronger form of function definition than is available in most programming languages; specifically, it allows the type of a result of a function application to depend on a formal parameter, the value (not merely the
type) of the input.

For formalizing long complex mathematical arguments Lean relies on \emph{computational reflection}. Dependent types make it possible for data, functions, and \emph{potential} computations to appear inside types. Standard mathematical practice is to interpret and expand these objects, replacing a constant by its definition, instantiating formal parameters, etc. while Lean supports this through typing rules that lets such computation happen transparently. 

\[\frac{t : A \quad A \equiv_{\beta \iota \delta \zeta} B}{t : B}\]
This rule states that the $\beta \iota \delta \zeta$-computation rules
of \cic yield equivalent types. It is a subsumption rule: there is no record of its use in the proof term $t$.

Arbitrarily long computations can thus be elided from a proof, as \cic has an $\iota$-rule for recursion. This yields an entirely new way of proving
results about specific calculations. With these tools at our disposal, we can quickly and efficiently formalize many of our results. Beyond merely certifying the correctness of our proofs, we expect that the proposed deliverables of this project will accelerate the pace of mathematical discoveries in our field.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Research methods} 
The most noteworthy methodological aspects of our project are listed below.
\begin{itemize}
\item \textsl{Collaborations:} Besides collaborating with our fellow mathematicians, we will also cooperate with computer scientists with expertise in Lean, including Jeremy Avigad (co-founder of Lean) and Jasmin Blanchette (PI of the ``Lean Forward'' project). Not only will they guide us through their field and help us carry out the formalization, they will provide frank feedback on the technology we develop and eventually integrate into the Lean Mathlib library~\cite{} 
\item \textsl{Robust engineering:} The software and libraries will be developed with maintainability in mind, so that they can serve in future research. To ensure their longevity, we will initiate a repository of third-party Lean formalizations and tools, inspired by the Archive of Formal Proofs [13].
\item \textsl{Flexiformality:} Russell quipped that ``the method of 'postulating' what we want has many advantages; they are the same as the advantages of theft over honest toil'' [60]. However, for most mathematicians, this fundamentalism is unappealing and unrealistic. Modern research on Diophantine equations often depends on the modularity of elliptic curves [18], which constitutes the final step in the proof of Fermat’s last theorem [69] and relies on an overwhelming amount of theory. Full formalization being completely unrealistic for the project, we must embrace flexible levels of formality [44].
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
This effort requires a careful reconsideration of how to express the informal 
logical foundations of our subject, since this determines how smoothly 
we can implement the core mathematical theory in the formal language of the proof assistant,
and this will in turn influence how accessible are the resulting libraries.

We are building tools that will make make the common, informal \emph{mathematical idioms}
available in Lean, which will make this software more accessible and make it 
easier to discover new theorems, verifying existing theorems,
and test conjectures, all using a language that is relatively close to the informal 
language that is commonly used by those of us working in universal algebra and lattice theory.

Our goal is to formally prove theorems and do research mathematics while at the same time 
addressing the main usability roadblocks that stand in the way of widespread 
adoption of proof assistant technology.  The theorems we decide to formalize and implement in the software
are selected, together with our collaborators, to guide the development of 
theorem libraries and verified tools and methods (or \emph{proof tactics}) for doing 
modern research in mathematics.  The main objective of this project 
(and others like it~\cite{lean-mathlib:2018,blanchette:2018}) is to develop and codify 
the \textbf{practical formal foundations of informal mathematics} 
in which most modern research is carried out.

To support the formalization of theorems, we will develop formal libraries of fundamental universal algebra
and lattice theory and explore how to automate proof search in these areas. We will
create libraries for algebraic structures, free algebras, clones of polynomials and term operations, 
and varieties (equational classes), and design and implement reasoning procedures for these.
We will explore how best to exploit Lean's \emph{metaprogramming} framework. Moreover, we will develop 
techniques and tools that help mathematicians carry out correct natural deduction and induction 
proofs using Lean, integrating procedures from existing computer algebra systems (e.g., UACalc) in a foundational, 
verified fashion. Finally, we will contribute to Lean's development with ideas and features designed
to benefit all users, notably an efficient built-in procedure for first-order reasoning and an integration 
with automatic theorem provers. The ultimate aim is to develop a proof assistant that actually helps 
mathematicians, by making them more productive and more confident in their results.

As scientists, we should take seriously any theory that 
may exposes weaknesses in our assumptions or our research habits.  We should
not be threatened by these disruptive forces; we must embrace them, deconstruct them,
and take from them whatever they can offer our mission of advancing pure mathematics.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%555
\newpage


\appendix

\section{Metadata}



\begin{table}[h]
  \begin{tabular}{|l|l|}
  \hline
  {\bf Title} & Formal Foundations for Informal Mathematics Research \\
  \hline
  % {\bf Subtitle} & Computer-aided proof and the Lean universal algebra library\\
  % \hline
  {\bf Primary Field} & 03C05 Equational classes, universal algebra\\ % (\msc\ {\small 2010: 03C05}) \\
  \hline
  {\bf Secondary Fields} & 
  03B35 Mechanization of proofs and logical operations\\ % % [See also 68T15]
  & 03B40 Combinatory logic and lambda-calculus\\ % % [See also 68N18]
  & 03F07  Structure of proofs\\
  & 03F55  Intuitionistic mathematics\\
  & 08B05 Equational logic, Malcev conditions\\
  & 08B20  Free algebras\\
  & 68N15  Programming languages\\
  & 68N18  Functional programming and lambda calculus \\ % [See also 03B40]
  & 68T15  Theorem proving (deduction, resolution, etc.)\\
  & 68W30  Symbolic computation and algebraic computation\\
  \hline
  \end{tabular}
  \end{table}
  




\subsection{Project Personnel}\
\vskip5mm
\noindent {\bf Principal Investigator.}

\vskip2mm
\hskip4.7mm {\bf William DeMeo} (Burnett Meyer Instructor, University of Colorado, Boulder)

\vskip5mm

\noindent {\bf Collaborators.}

\vskip2mm
\hskip4.7mm {\bf Clifford Bergman} (Professor, Iowa State University)

\vskip2mm
\hskip4.7mm {\bf Ralph Freese} (Professor, University of Hawaii)

\vskip2mm
\hskip4.7mm {\bf Peter Jipsen} (Professor, Chapman University)

\vskip2mm
\hskip4.7mm {\bf Connor Meredith} (Graduate Student, University of Colorado, Boulder)

\vskip2mm
\hskip4.7mm {\bf Hyeyoung Shin} (Graduate Student, Northeastern University)

\vskip2mm
\hskip4.7mm {\bf Siva Somayyajula} (Graduate Student, Carnegie Mellon University)



\vskip5mm

\noindent {\bf External Contributors and Advisors.}

\vskip2mm
\hskip4.7mm {\bf Jeremy Avigad} (Professor, Carnegie Mellon University)

\vskip2mm
\hskip4.7mm {\bf Andrej Bauer} (Professor, University of Ljubljana)

\vskip2mm
\hskip4.7mm {\bf Jasmin Blanchette} (Assistant Professor, Vrije Universiteit Amsterdam)


  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%% Some symbols: ℕ ℤ ∩ ⊂ ∀ ∃ Π α β γ ∈ ⦃ ⦄

\section{complementary projects}

\subsection{Lean Forward}
The Netherlands Organization for Scientific Research recently awarded 
a five-year grant 
to {\bf Jasmin Blanchette} for his \emph{Lean Forward} project, which is 
similar but complementary to ours. 
Dr.~Blanchette and his team are coming from computer science with support from mathematicians,
whereas we are coming from mathematics with support from computer scientists.  
Moreover, Blanchette works primarily with number theorists, whereas we are focused on 
universal algebra and lattice theory. We have had fruitful contact with Jasmin already 
and will attend his inaugural meeting in January 2019 in Amsterdamn.  
We look forward to productive future collaborations with the Lean Forward scientists.


\subsection{Existing libraries for universal algebra and lattice theory}
There has been prior work (mostly by computer scientists) on formalizing certain
parts of universal algebra. The first significant example of this was initiated in 
Venanzio Capretta's phd thesis (see~\cite{capretta:1999})
presenting the fundamentals of universal algebra using intuitionistic logic
so that the resulting theorems have computational meaning (via the propositions-as-types/proofs-as-programs
correspondence explained earlier).  Capretta's formalizations were done in Coq.

Another more recent development is 
the {\tt mathclasses} Coq library, initiated by Bas Spitters and 
Eelis van der Weegen~\cite{MR2678760}. 




\subsection{Formal proof archives}
The \href{https://www.isa-afp.org/}{Archive of Formal Proofs} is a collection of proof libraries,
examples, and larger scientific developments, mechanically checked in the theorem prover Isabelle.
It is organized in the way of a scientific journal, is indexed by dblp and has an ISSN: 2150-914x.
Submissions are refereed.. We encourage companion AFP submissions to conference and journal publications. 

A development version of the archive is available as well.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{More about Lean}

\subsection{Examples Demonstrating Lean's Ramified Hierarchy of Sorts and Types}
\label{sec:sorts-and-types}
~
\begin{lstlisting}
  universes u v
  variables A B : Prop
  variable α : Type u
  variable β : Type v
  variable γ : Type (u+1)

  #check A → B            -- Prop
  #check A → Prop         -- Type

  #check A → α            -- Type u
  #check α → α            -- Type u
  #check α → β            -- Type (max u v)
  #check α → γ            -- Type (u+1) 
  #check (α → β) → γ     -- Type (max (max u v) (u+1))

  #check α → Prop         -- Type u
  #check α → Type u       -- Type (u+1)
  #check α → Type v       -- Type (max u (v+1))

  #check Sort u → Sort u  -- Type u
  #check Type u → Type u  -- Type (u+1)
  #check Sort u → Sort v  -- Type (max u v)
  #check Type u → Type v  -- Type (max (u+1) (v+1))

  #check Prop              -- Type 
  #check Prop → Prop      -- Type 
  #check Sort              -- Type 
  #check Sort → Sort      -- Type 

  #check Type              -- Type 1
  #check Type → Type      -- Type 1
  #check Sort 1            -- Type 1
  #check Prop → Type      -- Type 1
  #check Sort 0 → Sort 1  -- Type 1

  #check Type 1            -- Type 2
  #check Type 1 → Prop    -- Type 2
  #check Type 1 → Sort 2  -- Type 2
\end{lstlisting}

%|||||||||||||||||||||||||||| OMIT BELOW |||||||||||||||||||||||||||||||||||||||
%VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV
\ifextver

\subsection{Lean's elaboration engine}
\label{app:elaboration}
%% source: Floris van Doorn post: https://homotopytypetheory.org/2015/12/02/the-proof-assistant-lean/
This section is partially based on part on an old blog post
of Floris van Doorn's that nicely summarizes some of Lean's key features.
    
On top of the Lean kernel there is a powerful \emph{elaboration engine} that can
\begin{enumerate}
\item infer implicit universe variables;
\item infer implicit arguments, using higher order unification;
\item support overloaded notation or declarations;
\item inserts coercions;
\item infers implicit arguments using type classes;
\item convert readable proofs to proof terms
\item constructs terms using tactics
\end{enumerate}
Lean does most of these things simultaneously. For example, the term constructed by
type classes can be used to find out implicit arguments for functions.

We now describe some of the elaboration engine features that appear in the list above.
\begin{enumerate}
\item {\bf Type inference.}
  Universe variables are rarely needed explicitly, and typically appear only when their absence would result in ambiguity.
  %% For example \lstinline{sum.code} as defined above could also live in \lstinline{Type.{(max u v)+3}} if the universe levels were not given explicitly.
  
\item As in Coq and Agda, in Lean we can use binders with curly braces $\{ \dots \}$ to indicate which arguments are left implicit.
  For example with the identity function above, if we write \lstinline{id (3 + 4)} this will be interpreted as \lstinline{@id _ (3 + 4)}.
  Then the placeholder \lstinline{_} will be filled by the elaborator to get \lstinline{@id ℕ (3 + 4)}. Lean also supports higher order
  unification. This allows, for example, to leave the type family of a transport implicit. For example (\lstinline{▸} denotes transport):

\begin{lstlisting}
  open eq
  variables (A : Type) (R : A → A → Type)
  variables (a b c : A) (f : A → A → A)
  example (r : R (f a a) (f a a)) (p : a = b) : R (f a b) (f b a) :=
  p ▸ r
\end{lstlisting}

Or the following lemma about transport in sigma-types:

\begin{lstlisting}
  open sigma sigma.ops eq
  variables {A : Type} {B : A → Type} {C : Πa, B a → Type} {a₁ a₂ : A}

  definition my_sigma_transport (p : a₁ = a₂)
  (x : Σ(b : B a₁), C a₁ b) : p ▸ x = ⟨p ▸ x.1, p ▸D x.2⟩ :=
  eq.rec (sigma.rec (λb c, idp) x) p
 
  set_option pp.notation false
  check @my_sigma_transport
  -- my_sigma_transport :
  --   Π {A : Type} {B : A → Type} {C : Π (a : A), B a → Type} {a₁ a₂ : A} (p : eq a₁ a₂) (x : sigma (C a₁)),
  --     eq (transport (λ (a : A), sigma (C a)) p x) (dpair (transport B p (pr₁ x)) (transportD C p (pr₁ x) (pr₂ x)))
\end{lstlisting}

Here the first transport is along the type family \lstinline{λa, Σ(b : B a), C a b}. The second transport is along the type
family \lstinline{B} and the \lstinline{p ▸D} is a dependent transport which is a map \lstinline{C a₁ b → C a₂ (p ▸ b)}. The check command
shows these type families. Also in the proof we don't have to give the type family of the inductions \lstinline{eq.rec} and
\lstinline{sigma.rec} explicitly. However, for nested inductions higher-order unification becomes expensive quickly, and
it is better to use pattern matching or the induction tactic, discussed below.

\item 
Lean supports overloading and coercions. Some examples:

\begin{lstlisting}
  open eq is_equiv equiv equiv.ops
  example {A B : Type} (f : A ≃ B) {a : A} {b : B}
  (p : f⁻¹ b = a) : f a = b :=
  (eq_of_inv_eq p)⁻¹
\end{lstlisting}

In this example \lstinline{f} is in the type of equivalences between \lstinline{A} and \lstinline{B} and is coerced into the function type.
Also, \lstinline{⁻¹} is overloaded to mean both function inverse and path inverse.

\item[5.] 
  As in Coq and Agda, Lean has \emph{type classes}, but in Lean they are very tightly integrated into the elaboration process.
  We use type class inference for inverses of functions. We can write \lstinline{f⁻¹} for an equivalence \lstinline{f : A → B},
  and then Lean will automatically try to find an instance of type \lstinline{is_equiv f}.

Here's an example, where we prove that the type of natural transformations between two functors is a set.
\begin{lstlisting}
import algebra.category.nat_trans
open category functor is_trunc
example {C D : Precategory} (F G : C ⇒ D)
  : is_hset (nat_trans F G) :=
is_trunc_equiv_closed 0 !nat_trans.sigma_char
\end{lstlisting}
In the proof we only have to show that the type of natural transformation is equivalent to a sigma-type (\lstinline{nat_trans.sigma_char})
and that truncatedness respects equivalences (\lstinline{is_trunc_equiv_closed}). The fact that the sigma-type is a set is
then inferred by type class resolution.

The brackets \lstinline{[...]} specify that that argument is inferred by type class inference.

\item[6.]
  There are multiple ways to write readable proofs in Lean, which are then converted to proof terms by the elaborator.
  For instance, we can define functions using pattern matching and (in contrast to Coq and Agda) these expressions
  are not part of the kernel. Instead, they are ``compiled down'' to basic recursors, keeping Lean's kernel simple,
  safe, and, well, \emph{lean.} Here is a simple example. The print command shows how \lstinline{inv} is defined internally.

\begin{lstlisting}
  open eq
  definition my_inv {A : Type} : Π{a b : A}, a = b → b = a
  | my_inv (idpath a) := idpath a
 
  print my_inv
  -- definition my_inv : Π {A : Type} {a b : A}, a = b → b = a :=
  -- λ (A : Type) {a b : A} (a_1 : a = b), eq.cases_on a_1 (idpath a)
\end{lstlisting}
  Here are some neat examples with vectors.

\begin{lstlisting}
  open nat
  inductive vector (A : Type) : ℕ → Type :=
  | nil {} : vector A zero
  | cons : Π {n}, A → vector A n → vector A (succ n)
 
  open vector
  -- some notation. The second line allows us to use
  -- [1, 3, 10] as notation for vectors
  notation a :: b := cons a b
  notation `[` l:(foldr `,` (h t, cons h t) nil `]`) := l
 
  variables {A B : Type}
 
  definition map (f : A → B) : Π {n : ℕ}, vector A n → vector B n
  | map [] := []
  | map (a::v) := f a :: map v
 
  definition tail : Π {n : ℕ}, vector A (succ n) → vector A n
  | n (a::v) := v
  -- no need to specify "tail nil", because that case is
  -- excluded because of the type of tail
 
  definition diag : Π{n : ℕ}, vector (vector A n) n → vector A n
  | diag nil := nil
  | diag ((a :: v) :: M) := a :: diag (map tail M)
  -- no need to specify "diag (nil :: M)"
 
  eval diag [[(1 : ℕ), 2, 3], [4, 5, 6], [7, 8, 9]]
  -- we need to specify that these are natural numbers
  -- [1,5,9]
\end{lstlisting}

You can use calculations in proofs, for example in the following construction of the composition of two natural transformations:

\begin{lstlisting}
  import algebra.category
  open category functor nat_trans
  variables {C D : Precategory} {F G H : C ⇒ D}
 
  definition nt.compose (η : G ⟹ H) (θ : F ⟹ G)
  : F ⟹ H :=
  nat_trans.mk
  (λ a, η a ∘ θ a)
  (λ a b f,
  calc H f ∘ (η a ∘ θ a) = (H f ∘ η a) ∘ θ a : by rewrite assoc
                    ... = (η b ∘ G f) ∘ θ a : by rewrite naturality
                    ... = η b ∘ (G f ∘ θ a) : by rewrite assoc
                    ... = η b ∘ (θ b ∘ F f) : by rewrite naturality
                    ... = (η b ∘ θ b) ∘ F f : by rewrite assoc)
\end{lstlisting}

There are various notations for using forward reasoning. For example the following proof is from the standard library.
In the proof below we use the notation \lstinline{p > 0}, which is interpreted as the inhabitant of the type \lstinline{p > 0}
in the current context. We also use the keyword this, which is the name term constructed by the previous unnamed have expression.

\begin{lstlisting}
  definition infinite_primes (n : nat) : {p | p ≥ n ∧ prime p} :=
  let m := fact (n + 1) in
    have m ≥ 1, from le_of_lt_succ (succ_lt_succ (fact_pos _)),
    have m + 1 ≥ 2, from succ_le_succ this,
    obtain p `prime p` `p ∣ m + 1`, from sub_prime_and_dvd this,
    have p ≥ 2, from ge_two_of_prime `prime p`,
    have p > 0, from lt_of_succ_lt (lt_of_succ_le `p ≥ 2`),
    have p ≥ n, from by_contradiction
      (suppose ¬ p ≥ n,
      have p < n, from lt_of_not_ge this,
      have p ≤ n + 1, from le_of_lt (lt.step this), have p ∣ m, from dvd_fact `p > 0` this,
      have p ∣ 1, from dvd_of_dvd_add_right (!add.comm ▸ `p ∣ m + 1`) this,
      have p ≤ 1, from le_of_dvd zero_lt_one this,
      absurd (le.trans `2 ≤ p` `p ≤ 1`) dec_trivial),
  subtype.tag p (and.intro this `prime p`)
\end{lstlisting}

\item[7.]  Lean has a tactic language similar to Coq's. For example, you can write
\begin{lstlisting}
  begin
    -- tactic based proof goes here -- 
  end
\end{lstlisting}
anywhere in a term to synthesize that subterm with tactics.
You can also use \lstinline{by foo} to apply a single tactic named \lstinline{foo}.
The type of the subterm you want to synthesize is the goal and you can use
tactics to solve the goal or apply backwards reasoning on the goal.
For example if the goal is \lstinline{f x = f y} you can use the tactic \lstinline{apply ap f} to reduce the goal to
\lstinline{x = y}. Another tactic is the \lstinline{exact} tactic, which means allows you to supply the proof term explicitly. 

The proof language offers various mechanisms to pass back and forth between the two modes.
You can begin using tactics anywhere a proof term is expected, and, conversely, you can enter
structured proof terms while in tactic mode using the exact tactic.

A very simple tactic proof is the following:
\begin{lstlisting}
  open eq
  variables {A B C : Type} {a a' : A}
  example (g : B → C) (f : A → B) (p : a = a')
  : g (f a) = g (f a') :=
  begin
    apply ap g,
    apply ap f,
    exact p
  end
\end{lstlisting}
which produces the proof term \lstinline{ap g (ap f p)}. In the Emacs mode of Lean you can see the
intermediate goals by moving your cursor to the desired location and pressing \lstinline{Ctrl-C Ctrl-G}.

Lean has too many tactics to discuss here, although not as many as Coq.
Here are some neat examples of tactics in Lean.

The cases tactic can be used to destruct a term of an inductive type. In the following examples,
it destructs the path \lstinline{p} to reflexivity. In the second example it uses that \lstinline{succ} is
injective, since it is the constructor of \lstinline{nat}, so that it can still destruct \lstinline{p}.
It also doesn't matter whether the free variable is on the left hand side or the right hand
side of the equality. In the last example it uses that different constructors of an inductive type cannot be equal.

\begin{lstlisting}
  open nat eq
  example {A : Type} {x y : A} (p : x = y) : idp ⬝ p = p :=
  begin cases p, reflexivity end
  
  example (n m l : ℕ) (p : succ n = succ (m + l))
  : n = m + l :=
  begin cases p; reflexivity end
  
  example (n : ℕ) (p : succ n = 0) : empty :=
  by cases p
\end{lstlisting}

The \lstinline{rewrite} tactic is useful for doing a lot of rewrite rules.
It is modeled after the \lstinline{rewrite} tactic of SSReflect, the library that originated with
the project to formalize the (very long) proofs of the Four Color and the Feit-Thompson
Theorem. For example in the following proof of Eckmann-Hilton we rewrite the goal 4
times with theorems to solve the goal. The notation \lstinline{▸*} simplifies the goal
similar to Coq's \lstinline{simpl}, and \lstinline{-H} means to rewrite using \lstinline{H⁻¹}.

\begin{lstlisting}
  open eq
  theorem eckmann_hilton {A : Type} {a : A}
  (p q : idp = idp :> a = a) : p ⬝ q = q ⬝ p :=
  begin
    rewrite [-whisker_right_idp p, -whisker_left_idp q, ▸*,
      idp_con, whisker_right_con_whisker_left p q]
  end
\end{lstlisting}

The \lstinline{induction} tactic performs induction, naturally. For example if the goal is \lstinline{P n} for
a natural number \lstinline{n} (which need not be a variable), then you can use \lstinline{induction n} to
obtain the two goals \lstinline{P 0} and \lstinline{P (k + 1)} assuming \lstinline{P k}. What's really neat is
that it supports user-defined recursors. So you can define the recursor of a HIT, tag it
with the \lstinline{[recursor]} attribute, and then it can be used in the \lstinline{induction} tactic.
Even more: you can arrange it in such a way that Lean will use the \emph{nondependent recursion principle}
whenever possible, falling back to the \emph{dependent recursion principle} when necessary.

\begin{lstlisting}
  import homotopy.circle types.int
  open circle equiv int eq pi
  
  definition circle_code (x : S¹) : Type₀ :=
  begin
    induction x, -- uses the nondependent recursor
    { exact ℤ},
    { apply ua, exact equiv_succ}
    -- equiv_succ : ℤ ≃ ℤ with underlying function succ
  end
  
  definition transport_circle_code_loop (a : ℤ)
  : transport circle_code loop a = succ a :=
  ap10 !elim_type_loop a
  -- ! is the "dual" of @, i.e. it inserts
  -- placeholders for explicit arguments
  
  definition circle_decode {x : S¹}
  : circle_code x → base = x :=
  begin
    induction x, -- uses the dependent recursor
    { exact power loop},
    { apply arrow_pathover_left, intro b,
    apply concato_eq, apply pathover_eq_r,
    rewrite [power_con,transport_circle_code_loop]}
  end
\end{lstlisting}

In the future there will be more tactics, providing powerful automation such as a simplifier and a tableaux theorem prover.
\end{enumerate}
\fi
%/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\
% | | | | | | | | | | | |  OMIT ABOVE | | | | | | | | | | | | | | | | | | | | 







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%|||||||||||||||||||||||||||| OMIT BELOW |||||||||||||||||||||||||||||||||||||||
%VVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVVV
\ifextver

\section{other research activities of William DeMeo}
\label{sec:other}
This section mentions a few other projects in which I am actively engaged.

\subsection{Finitely generated free lattices and bounded homomorphisms}
\label{sec:fg-free-lat}
\noindent {\bf TODO: write this section}

\subsection{Computational complexity of computing difference terms}
\label{sec:software-development}
One successful project branch of this project investigates classical computational problems
in algebra in order to determine whether they are algorithmically solvable
and, if so, whether they are computationally \emph{tractable}.
For example, in my most recent work with Ralph Freese and Matthew Valeriote,
we proved that a particular type of term called a \emph{difference term}

We recently developed algorithms that solve these problems and 
incorporate them into the UACalc software, a proof assistant developed
by Ralph Freese to handle computations involving algebraic structures. 
%% The researchers shall work to decide the truth of the CSP Dichotomy Conjecture
%% of Feder and Vardi, which states that every Constraint Satisfaction Problem with
%% a finite template is solvable in polynomial time or is NP complete. They will
%% further develop the algebraic approach to CSP's by refining knowledge about
%% relations compatible with weak idempotent Maltsev conditions and about algebras
%% with finitely related clones. 
%% Another aspect of the project concerns the computable recognition of properties
%% of finite algebras connected with the varieties they generate, such as whether a
%% finite algebra with a finite residual bound is finitely axiomatizable, or
%% whether a finite algebra can serve as the algebra of character values for a
%% natural duality.
Thus, one of the more tangible outcomes of the project will be
increasingly powerful and more broadly applicable software for solving
algebra problems, both theoretical and applied.
The agenda for this part of the project includes parallelizing the important subroutines,
building in conjecture-testing and search features, adding further algorithms,
and further developing the community of users and contributors. An additional
effort to incorporate some of the latest technologies developed in the Type
Theory and Programming Lagnuages research community is also under way.



\subsection{Computational Universal Algebra}
The Universal Algebra Calculator (UACalc) was initiated by Emil Kiss and Matthew Valeriote,
and has been developed by Ralph Freese for the past two decades.  In that time 
the UACalc has grown to include many very powerful tools for analyzing various 
classes of algebraic structures and revealing their properties in a computationally 
tractable way whenever possible.  UACalc is desinged to be used through a  
graphical user interface (GUI), which is ideal for researchers with little programming experience 
and for those situations in which we are interested in a single algebraic structure 
whose (Cayley) operation tables we don't mind entering by hand. 
However, there are times when a graphical interface is less than ideal.

Fortunately, a few years ago we realized that we could gain access the UACalc's
large and mature library of algorithms from the command line using either the 
Scala or Python interpreter.\footnote{Specifically, the implementation of Python that we 
use is called Jython, which runs on the Java Virtual
Machine (JVM).} Moreover, it's easy to import  UACalc Java packages
into Scala and Python programs, so anyone who knows a little Scala or Python 
can write simple scripts to test conjectures about large collections of
algebras, without having to manually enter each algebra into the UACalc
GUI.

For example, one powerful feature of the UACalc that Ralph Freese recently
implemented is an efficient method for quickly testing whether a finite
idempotent algebra generates a congruence permutable variety.  (See
\cite[Theorem 5.1]{Freese:2009}.)  With a simple script we can now apply
this method to a large batch of algebras and almost instantly know which ones
generate congruence permutable varieties.

One of the current goals of DeMeo, Freese, and Connor Meredith 
(a graduate student at CU) is to parallelize some of the more
important UACalc subroutines that, in their current serial implementation, can
cause bottlenecks when using the software, even when the 
algebras involved are small.  One example is the subroutine that 
computes the subalgebra generated by a given finite set.  Freese has written a 
parallel version of this routine and his preliminary tests show that the 
speedup is significant, despite the fact that the general problem of computing 
a subalgebra is well known to be complete for the class of problems solvable in 
polynomial time.

\subsection{Atlas of congruence lattices of finite algebras}
\label{sec:atlas-congr-latt}
\noindent {\bf TODO: update this section}

In ongoing work with Ralph Freese and Peter Jipsen, we are developing an
atlas of congruence lattices of finite algebras. The goal is to have dynamic
document served by a database that organizes and catalogs all known congruence
lattices of finite algebras. The document will be dynamic in the sense that it
and the underlying database will exist in a publicly available GitHub repository
that can be cloned and updated by various researchers around the world.  These
researchers can submit ``pull requests'' for us to consider including in the
official atlas.  In this way, we hope it will reflect, to a close approximation,
the current state of knowledge about finite congruence lattices.  




\fi
%/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\
% | | | | | | | | | | | |  OMIT ABOVE | | | | | | | | | | | | | | | | | | | | 



% \subsection{web presence}
% \label{sec:universalalgebra.org}
% About five years ago I started the website \url{http://universalalgebra.org}
% as well as the Universal Algebra GitHub organization.
% My motivation is to provide a useful platform for our research community.
% On resource that seems to be particularly useful to others is the list of
% ``universal algebra collegues'' that I maintain in that repository.
% This list has been used to advertised conferences and job opportunities, such as the
% one for which I'm presently applying.)

%% Tim Gowers has shown how effective a simple WordPress blog
%% can be for massively collaborative mathematics (see
%% \url{http://gowers.wordpress.com/2009/01/27/}), and such collaborations
%% surely have a place in our area as well.  I have enlisted the help of
%% Ralph Freese and Peter Jipsen as the first editors of the site.

%% Currently the main function of universalalgebra.org is to host the
%% web page for the 2013 Workshop in Computational Universal Algebra.








%% {\bf Near-term goals for algebraic CSP work:} 
%% \begin{enumerate}
%% \item
%% {\bf Settle the CSP-dichotomy conjecture for finite \cibs.} To achieve this,
%% it remains to show that a finite \cib with a proper semilattice  
%% divisor is tractable. To begin with, DeMeo used the Universal Algebra Calculator to find
%% all four-element \cib that have the two-element semilattice as a subalgebra. 
%% All but 32 of these algebras generate a congruence $\mbox{SD}_\wedge$  variety
%% (so the associated \csp is known to be solvable by the ``local consistency algorithm'').
%% Therefore, we will study the 32 algebras that do not generate congruence
%% $\mbox{SD}_\wedge$ varieties, starting with the simple ones, and attempt to
%% find new polynomial-time algorithms that solve the \csps associated with
%% these algebras.
%% \end{enumerate}





\bibliographystyle{plainurl}
\bibliography{aux/refs}

\end{document}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%         END         %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%       DOCUMENT      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%










%% \providecommand{\bysame}{\leavevmode\hbox to3em{\hrulefill}\thinspace}
%% \providecommand{\MR}{\relax\ifhmode\unskip\space\fi MR }
%% % \MRhref is called by the amsart/book/proc definition of \MR.
%% \providecommand{\MRhref}[2]{%
%%   \href{http://www.ams.org/mathscinet-getitem?mr=#1}{#2}
%% }
%% \providecommand{\href}[2]{#2}
%% \begin{thebibliography}{1}

%% \bibitem{BergmanFailing2013}
%% C.~Bergman and D.~Failing, \emph{Commutative, idempotent groupoids and the
%%   constraint satisfaction problem}, submitted to Algebra Universalis.

%% \bibitem{Maltsev1967}
%% A.~I. Mal'cev, \emph{Multiplication of classes of algebraic systems}, Siberian


%%   Math. J. \textbf{8} (1967), 254--267.

%% \bibitem{Neumann1967}
%% H.~Neumann, \emph{Varieties of groups}, Springer--Verlag, Berlin, 1967.







% \def\cprime{$'$} \def\cprime{$'$} \def\cprime{$'$}
%   \def\ocirc#1{\ifmmode\setbox0=\hbox{$#1$}\dimen0=\ht0 \advance\dimen0
%   by1pt\rlap{\hbox to\wd0{\hss\raise\dimen0
%   \hbox{\hskip.2em$\scriptscriptstyle\circ$}\hss}}#1\else {\accent"17 #1}\fi}
% \providecommand{\href}[2]{#2}
% \providecommand{\bysame}{\leavevmode\hbox to3em{\hrulefill}\thinspace}
% \providecommand{\MR}{\relax\ifhmode\unskip\space\fi MR }
% % \MRhref is called by the amsart/book/proc definition of \MR.
% \providecommand{\MRhref}[2]{%
%   \href{http://www.ams.org/mathscinet-getitem?mr=#1}{#2}
% }
% \begin{thebibliography}{1}
% 
% \bibitem{BergmanFailing2013}
% C.~Bergman and D.~Failing, \emph{Commutative, idempotent groupoids and the
%   constraint satisfaction problem}, submitted to Algebra Universalis.
% 
% \bibitem{Maltsev1967}
% A.~I. Mal'cev, \emph{Multiplication of classes of algebraic systems}, Siberian
%   Math. J. \textbf{8} (1967), 254--267.
% 
% \bibitem{Neumann1967}
% H.~Neumann, \emph{Varieties of groups}, Springer--Verlag, Berlin, 1967.
% 
% \bibitem{Escardo:2008}
% M.~Escard{\'{o}}, \emph{Exhaustible sets in higher-type
%   computation}, Logical Methods in Computer Science \textbf{4} (2008), no.~3.
% 
% \bibitem{Freese:2009}
% R.~Freese and M.~Valeriote, \emph{On the complexity of some
%   {M}altsev conditions}, Internat. J. Algebra Comput. \textbf{19} (2009),
%   no.~1, 41--77. \MR{2494469 (2010a:08008)}
% 
% \bibitem{KearnesTschantz:2007}
% K.~Kearnes and S.~Tschantz, \emph{Automorphism groups of squares and of free algebras},
% Internat. J. Algebra Comput. \textbf{17} (2007),
% no.~3, 461--505 \MR{08A35 (08B20 20B25)}
% 
% 
% 
% Conjecture. A variety of CIB's disjoint from the variety of semilattices
% is congruence permutable.
% 
% This conjecture is true. Proof follows.
% -------------------------------------------------------------
% -------------------------------------------------------------
% Lm 2.8 from page 7 of Paper #63 on my website:
% Let V be an idempotent variety that is not congruence permutable. If
% F = F_V(x,y) is the 2-generated free algebra in V, then F has
% subuniverses U and V such that
% (1) x \in U, x \in V ,
% (2) y \notin U, y \notin V , and
% (3) (UxF) \cup (FxV) is a subuniverse of FxF.
% -------------------------------------------------------------
% 
% Proof of the Conjecture:
% 
% Let V be a variety of CIB's that is not CP.
% Let x, y, F, U, V be as in Lemma 2.8.
% 
% Case 1:
% there are elements u\in U; v\in V; f, g\in F such that
% u*f \notin U and g*v \notin V.
% In this case, (u,g)\in UxF and (f,v)\in FxV, but the product
% is not in (UxF)\cup(FxV), contradicting the fact that this union
% is a subuniverse.
% 
% Thus Case 1 cannot occur, which means U*F\subseteq U or
% F*V\subseteq V. Assume the former, that is, U*F\subseteq U.
% Using the commutativity of the binary operation, we are
% led to the alternative case
% 
% Case 2: U is an ideal. (U*F=F*U \subseteq U).
% 
% We still have x\in U and y\notin U. Since U is an ideal,
% S := U\cup \{y\} is a subuniverse of F.
% Since U is an ideal, the subuniverse S has a congruence
% \theta with 2 nonempty blocks, U and {y}.
% The quotient S/\theta is a 2-element semilattice,
% which belongs to V. \\\\
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% Let us recall that NP denotes the family of algorithmic problems that are polynomial-time reducible
% to the digraph three-coloring problem. NP-complete consists of those problems in NP to which digraph
% three-coloring can be polynomial-time reduced. P is the family of problems in NP that can be solved by
% a deterministic algorithm working in polynomial time. Richard Ladner [29] showed that if P = NP then
% there is a densely ordered set of polynomial-time reducibility classes of problems in NP that are neither
% P nor NP-complete. The \ac{CSP}-dichotomy conjecture of Tom ́
% as Feder and Moshe Vardi [18] states that no
% such problem can be found in the family CSP; i.e., for each R, CSP(R) is either NP-complete, or in P.
% The conjecture has resisted all efforts and continues to be plausible. The work on this conjecture has
% wonderfully energized several research communities. The universal algebraic approach to this conjecture
% has been especially successful, producing a rich harvest of results and insights, in algebra as well as in
% complexity theory. Many conferences and workshops over the past decade, for example at Vanderbilt
% (2007), the American Institute of Mathematics (2008), the Fields Institute (2011), Dagstuhl Institute
% (Germany) (2012) and Banf International Research Station (2014) had or will have, as a prominent theme,
% the algebraic approach.
% Algebra becomes relevant through these observations of Jeavons, Krokhin and Bulatov [14]: A rela-
% tional structure R = A, · · · has its relational clone Clo(R) of derived relations, which is the smallest
% set of relations over A containing the given relations, the trivial relations, and closed under intersection,
% concatenation (or product), permutations of variables, and projections. A universal algebra A = A, · · ·
% has its operational clone Clo(A) of derived operations, which is the smallest set of operations containing
% the given ones, and the trivial (projection) operations, and closed under all compositions.
% The relational structure R also defines a clone of operations on A, and an algebra
% A(R) = A, Poly(R)
% where Poly(R) (the set of polymorphisms of R) is the set of all operations on A that respect the given
% relations or, in other terms, Poly(R) is the collection of all homomorphisms R n → R (for any non-negative
% integer n). The algebra A also defines a relational structure
% R(A) = A, Rel(A)
% where Rel(A) is the set of all relations that respect the given operations (the set of admissible relations of
% A), or in other terms, is the set of all subuniverses of finite powers of A.
% When A is finite, the Galois connection between relations and operations is exact. Indeed, an old and
% easy result states that
% Rel(A(R)) = Clo(R) and Poly(R(A)) = Clo(A), if A is finite.R. McKenzie
% 13
% Now if R = A, R 1 , . . . , R k is as above with A finite, then it is easy to see that for every structure R =
% A, S 1 , . . . , S m with {S 1 , . . . , S m } ⊆ Clo(R), CSP(R ) admits a polynomial time reduction to CSP(R).
% If B is any finite algebra in the variety generated by A(R), then for every relational structure S =
% B, T 1 , . . . , L u formed from relations in Rel(B), there is a polynomial time reduction of CSP(S) to CSP(R).
% The algorithmic complexity of CSP(R) is thus defined by the algebra A(R), and persists as a bound on
% the complexity of all CSP problems residing anywhere in the variety generated by A(R). If any “difficult”
% set of relations appears among the admissible relations on some finite algebra in the variety generated by
% A(R), then CSP(R) is forced to be NP-complete. On the other hand, the non-occurence of such difficult
% sets is likely to force R to have some interesting polymorphisms, and we can hope to use these operations
% to fashion a polynomial-time algorithm for resolving CSP(R). We now discuss at some length how these
% observations have proved their worth.
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% 
% Due to a rich body of recent work in the area of algebraic CSP, the algebraic
% version of the \emph{\csp-dichotomy conjecture} now boils down to 
% the following assertion:
% \begin{quote}
% \emph{If a finite idempotent algebra $\mathbf{A}$ generates a variety with a weak near
% unanimity term operation, then the associated constraint satisfaction problem 
% $\mathrm{CSP}(\mathbf{A})$ is tractable.}
% \end{quote}
% (The converse of this statement is known to be true.)
% 
% It follows directly from the definition that a binary operation is a weak
% near-unanimity operation if and only if it is idempotent and commutative. This
% suggests the following question: Is every finite commutative idempotent binar
% tractable? (A \emph{binar} is an algebra with a single binary basic operation.)
% We denote the variety of commutative idempotent binars by \cib. 
% 
% By applying {\'A}gnes Szendrei's characterization of finite, idempotent, strictly simple
% algebras in~\cite[Thm.~2.1]{MR911575}, %% Szendrei:1987}), 
% Bergman and DeMeo proved that every locally finite, equationally complete
% variety of \cibs, except for semilattices, is congruence-permutable (hence, has
% a tractable CSP). Moreover, Bergman proved that such varieties are pairwise
% independent. Consequently, the join of any two of these minimal varieties is
% congruence-permutable. It follows from this, together with techniques
% from~\cite{MR3350338}, that the join of any two minimal varieties of
% \cibs has a tractable CSP.  
% 
% 
% \vskip2cm
% 
% 
% 
% 
% 
% \section{distinguishing characteristics of popular proof assistants}
% We now briefly discuss the ways in which Lean differs from other popular proof assistants.
% Some close relatives of Lean are Agda, Coq, and \nuprl.
% All four of these languages are founded upon some interpretation of intuitionistic type theory (\itt).
% %(Isabelle is based on \fol and \hol.)
% 
% 
% \begin{center}
%   \begin{figure}
% \begin{tabular}{|r|c|c|c|}
%                   &  {\bf extensional} & {\bf quasi-intensional} & {\bf intentional} \\
%   \hline
%   {\bf predicative} &       \href{http://www.nuprl.org/}{\nuprl}          &                 &             \\
%   \hline
%   {\bf quasi-predicative} &             &   \href{https://leanprover.github.io}{Lean}         &     \\
%   \hline
%   {\bf impredicative} &                    &              &     \\
%   \end{tabular}
% \caption{Comparison of paradigms of some popular proof assistants}
%   \end{figure}
%   \end{center}  
% 
% Coq and Lean both have an impredicative Prop type as well as a predicative type hierarchy.
% The difference is that 
% 
% \begin{center}
%   \begin{figure}
% \begin{tabular}{|r|l|l|l|l|}
% {\bf Language}   &  {\bf style of logic } & {\bf predicative} & {\bf extensional} & {\bf origin} \\
%   \hline
%   \href{http://wiki.portal.chalmers.se/agda}{Agda} &  & &                                                       & 2007, Chalmers, SE \\
%   \hline
%   \href{https://coq.inria.fr/}{Coq}   &  & &                                                     &     IRNIA, FR  \\
%   \hline
%   \href{https://isabelle.in.tum.de/}{Isabelle}     &  meta-logic (a weak \TT) & & &1986, Cambridge, GB; TU Munich, DE\\
%   \hline
%   \href{https://leanprover.github.io}{Lean}   &  & &                                                    & Microsoft and CMU, US \\
%   \hline
%   \href{http://www.nuprl.org/}{NuPrl}  &  & &                                                   & Cornell, US \\
%   \end{tabular}
% \caption{Some properties five popular proof assistants based on intuitionistic type theory}
%   \end{figure}
%   \end{center}  
% Initial release	[1]
% 
% \begin{center}
%   \begin{figure}
% \begin{tabular}{|r|c|c|c|c|c|c|c|c|}
% {\bf Language} & {\bf Paradigm} & {\bf Predicative} & {\bf Extensional} & {\bf Tactics} & {\bf Proof terms} & {\bf Termination checking} & {\bf Proof irrelevance} & {\bf Program extraction}\\
% \hline
%   \href{http://wiki.portal.chalmers.se/agda}{Agda}   &     \utt          &            &        &                  & No &                                      & 2007, Chalmers, SE& \\
%   \hline
%   \href{https://coq.inria.fr/}{Coq}                      &  \cic & &                                         &            &     IRNIA, FR & \\
% \href{https://isabelle.in.tum.de/}{Isabelle}         &  meta-logic (a weak type theory), &&&&\\
%     \hline
%   \href{https://leanprover.github.io/}{Lean}   & \cic  & &                                     &               & Microsoft and CMU, US & \\
%   \hline
%       \href{http://www.nuprl.org/}{NuPrl}   & MLTT & &                                  &                 & Cornell, US & \\
%       \hline
% \end{tabular}
%   \end{figure}
% \end{center}
% 
% 
% \subsection{Agda}
% %% http://www.cse.chalmers.se/~ulfn/papers/thesis.pdf
% {\bf Proof automation.}
% Programming in pure type theory involves a lot of tedious and repetitive proofs, and Agda has no support for tactics. Instead, Agda has support for automation via \emph{reflection}.
% The reflection mechanism allows one to quote program fragments into---or unquote them from---the abstract syntax tree.
% Another mechanism for proof automation is proof search action in emacs mode. It enumerates possible proof terms (limited to 5 seconds), and if one of the terms fits the specification, it will be put in the meta variable where the action is invoked. This action accepts hints, e.g., which theorems and from which modules can be used, whether the action can use pattern matching, etc.[9]
% 
% {\bf Termination checking.}
% Agda is a total language, i.e., each program in it must terminate and all possible patterns must be matched. Without this feature, the logic behind the language becomes inconsistent, and it becomes possible to prove arbitrary statements. For termination checking, Agda uses the approach of the Foetus termination checker.[10]
% 
% 
% \subsection{Isabelle}
% The \href{https://isabelle.in.tum.de/}{Isabelle theorem prover} is an interactive theorem prover, a Higher Order Logic (HOL) theorem prover. It is an LCF-style theorem prover (written in Standard ML), so it is based on a small logical core to ease logical correctness. Isabelle is generic: it provides a meta-logic (a weak type theory), which is used to encode object logics like first-order logic (FOL), higher-order logic (HOL) or Zermelo–Fraenkel set theory (ZFC). Isabelle's main proof method is a higher-order version of resolution, based on higher-order unification. Though interactive, Isabelle also features efficient automatic reasoning tools, such as a term rewriting engine and a tableaux prover, as well as various decision procedures.
% 
% Isabelle has been used to formalize numerous theorems from mathematics and computer science, like G\"odel's completeness theorem, G\"odel's theorem about the consistency of the axiom of choice, the prime number theorem, correctness of security protocols, and properties of programming language semantics. The Isabelle theorem prover is free software, released under the revised BSD license.
% 
% 
% 
% 
% 
% 













\subsection{Extensional vs.~Intensional}
A fundamental distinction in type theory is that of \emph{extensional} vs \emph{intensional}.
In \emph{extensional type theory}, definitional (i.e., computational) equality is not distinguished
from propositional equality, which requires proof.
For example, \emph{function extensionality} means that two functions $f$ and $g$ are
indistinguishable if they give the same output for every input. (This is sometimes called ``Leibniz equality.'')
Briefly, \emph{intensional} means not extensional, but let us elaborate.
An \emph{intensional definition} of a term gives the meaning of a term by specifying necessary 
and sufficient conditions that such a term satisfies.  In the case of nouns, this is equivalent 
to specifying all the properties that an object must have in order to be something to which the term refers.

If we adopt an extensional type theory, then one undesirable consequence is that \emph{type-checking is undecidable}
because programs in the theory might not terminate.\footnote{For example, one can define 
the Y-combinator in extensional type theory; for a detailed example, see~\cite{NPS:1990}.}
Of course, a program that doesn't terminate is practically indistinguishable from a program
that takes many lifetimes to terminate, and some practical proof assistants---like
Lean and \nuprl---employ extensional types in certain instances where it is safe and sensible to do so.

\emph{Intensional type theory} (\itt) enjoys decidable type-checking, but 
the cost is that representing some important mathematical objects can be cumbersome.
In particular, intensional reasoning relies on constructions such as \emph{setoids} to
represent classes of objects that are (intensionally) equivalent. Consequently,
encoding certain mathematical objects in \itt can be inelegant and difficult to work with,
in contrast to their simple informal representations.
Some examples are integers, rational numbers, and real numbers.\footnote{Although
  integers and rational numbers can be represented in \itt without setoids, the
  representation is not very easy to work with; real numbers cannot be represented in \itt
  without setoids.~\cite{MR3396201,MR2163416}.
%% [3] Altenkirch, Thorsten, Thomas Anberrée, and Nuo Li. "Definable Quotients in Type Theory."
Homotopy type theory works on resolving this problem. It allows one to define higher inductive types,
which define not only first order objects (values or points), but also higher order objects,
e.g., equalities between elements, equalities between equalities (homotopies), etc.}




\subsection{Predicative vs.~Impredicative}
A self-referencing definition is called \emph{impredicative}. More precisely, 
a definition is said to be impredicative if it invokes (mentions or quantifies over) the set being defined, 
or (more commonly) another set that contains the thing being defined.
An (in)famous example of an impredicative construction is Russel's set of all sets that do not contain themselves,
but there are less ``exotic'' examples. Even the \emph{greatest lower bound} of a set $X$,
$\bigwedge X$, has an impredicative definition: $y = \bigwedge (X)$ if and only if for all elements
$x$ of $X$, $y$ is less than or equal to $x$, and any $z$ less than or equal to all elements of $X$ is less than or equal to $y$.
This definition quantifies over the set whose members are the lower bounds of $X$, one of which is
the greatest lower bound itself.
%% ---namely, the set of all sets that do not contain themselves. The paradox is that such a set cannot exist: If it would exist, the question could be asked whether it contains itself or not---if it does then by definition it should not, and if it does not then by definition it should.

The opposite of impredicativity is \emph{predicativity}, which depends on a 
\emph{stratified} (or \emph{ramified}) theory where quantification over lower levels
results in variables of some new type of higher type, separate from the lower types
over which the variable ranges. A prototypical example is Per Martin-L\"of's
\emph{intuitionistic type theory} (\itt),
which provides the theoretical basis for most of the popular modern proof assistants
in use today.\footnote{In fact, Martin-L\"of modified his type theory a number of times;
  his 1971 impredicative formulation was shown to be inconsistent by Girard's paradox,
  and his later formulations were predicative. Martin-L\"of proposed both intensional and extensional variants of the theory.}


% \subsection{Set theory vs. type theory}
% There are many different set theories and many different systems of type theory. Nonetheless, this brief section describes a few general principles.
% \begin{itemize}
% \item Set theory is built on top of logic. It requires a separate system like \emph{predicate logic} underneath it.
% In type theory, concepts like ``and'' and ``or'' can be encoded as types in the type theory itself.
% \item In set theory, an element can belong to multiple sets, either to a subset or to a superset.
% In type theory, terms (generally) belong to only one type. (Where a subset would be used, type
% theory tends to use a predicate function that returns true if the term is in the subset and returns
% false if the value is not. The union of two types can be done by creating a new type called a sum type,
% which contains new terms.)
% \item Set theory usually encodes numbers as sets. (0 is the empty set, 1 is a set containing the empty set, etc.)
% Type theory can encode numbers as functions using Church encoding or more naturally as inductive types.
% Inductive types create new constants for the successor function and zero, and closely resembles Peano's axioms.
% \item Set theory allows set builder notation.  Type theory has a simple connection to constructive mathematics through
% the BHK interpretation. It can be connected to logic by the Curry Howard isomorphism. And some type theories are closely connected to Category theory.
% \end{itemize}
